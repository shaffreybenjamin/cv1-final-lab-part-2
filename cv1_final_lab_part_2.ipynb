{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzW_uc8bvtFj"
   },
   "source": [
    "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1200/1*lbDXL0IuitCRz4mpZ7MmfQ.png\" width=55% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "    <font size=\"6\">Final Lab (Part 2): Image Classification using Convolutional Neural Networks </font>\n",
    "</center>\n",
    "<center>\n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font>\n",
    "</center>\n",
    "<center>\n",
    "    <font size=\"4\">Due 23:59PM, October 18, 2024 (Amsterdam time)</font>\n",
    "</center>\n",
    "<center>\n",
    "    <font size=\"4\"><b>TA's:  Yue, Konrad & Thies</b></font>\n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  \\\n",
    "Student1 Name:\n",
    "\n",
    "Student2 ID: \\\n",
    "Student2 Name:\n",
    "\n",
    "Student3 ID: 13853163\\\n",
    "Student3 Name Benjamin Shaffrey:\n",
    "\n",
    "Student4 ID: 15050025\\\n",
    "Student4 Name: Christina Isaicu\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-qY1CUhvtFn"
   },
   "source": [
    "### **Coding Guidelines**\n",
    "\n",
    "Your code must be handed in this Jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Final Lab: Image Classification Assignment. Please also fill out your names and IDs above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Understand the problem as much as you can. When answering a question, provide evidence (qualitative and/or quantitative results, references to papers, figures, etc.) to support your arguments. Not everything might be explicitly asked for, so think about what might strengthen your arguments to make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Add a number, a title, and, if applicable, the name and unit of variables in a table, and name and unit of axes and legends in a figure.\n",
    "\n",
    "**Late submissions are not allowed.** Assignments submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance to avoid last-minute system failure issues.\n",
    "\n",
    "**Environment:** Since this is a project-based assignment, you are free to use any feature descriptor and machine learning tools (e.g., K-means, SVM). You should use Python for your implementation. You are free to use any Python library for this assignment, but make sure to provide a conda environment file!\n",
    "\n",
    "**Plagiarism Note:** Keep in mind that plagiarism (submitted materials which are not your work) is a serious offense and any misconduct will be addressed according to university regulations. This includes using generative tools such as ChatGPT.\n",
    "\n",
    "**Ensure that you save all results/answers to the questions (even if you reuse some code).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh1MdIwavtFo"
   },
   "source": [
    "### **Report Preparation**\n",
    "\n",
    "Your tasks include the following:\n",
    "\n",
    "1. **Report Preparation:** For both parts of the final project, students are expected to prepare a report. The report should include all details on implementation approaches, analysis of results for different settings, and visualizations illustrating experiments and performance of your implementation. Grading will be based on the report, so it should be as self-contained as possible. If the report contains faulty results or ambiguities, TAs can refer to your code for clarification.\n",
    "\n",
    "2. **Explanation of Results:** Do not just provide numbers without explanation. Discuss different settings to show your understanding of the material and processes involved.\n",
    "\n",
    "3. **Quantitative Evaluation:** For quantitative evaluation, you are expected to provide the results based on performance (accuracy, learning loss and learning curves).\n",
    "\n",
    "4. **Aim:** Understand the basic Image Classification pipeline using Convolutional Neural Nets (CNN's).\n",
    "\n",
    "5. **Working on Assignments:** Students should work in assigned groups for **two** weeks. Any questions can be discussed on ED.\n",
    "\n",
    "    - **Submission:** Submit your source code and report together in a zip file (`ID1_ID2_ID3_part2.zip`). The report should be a maximum of 10 pages (single-column, including tables and figures, excluding references and appendix). Express thoughts concisely. Tables and figures must be accompanied by a description. Number them and, if applicable, name variables in tables, and label axes in figures.\n",
    "\n",
    "6. **Hyperparameter Search:** In your experiments, remember to perform a hyperparameter search to find the optimal settings for your model(s). Clearly document the search process, the parameters you explored, and how they influenced the performance of your model.\n",
    "\n",
    "8. **Format and Testing:** The report should be in **PDF format**, and the code in **.ipynb format**. Test that all functionality works as expected in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fp3zpjcvtFp"
   },
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Image Classification on CIFAR-100 (0 points)](#section-1)\n",
    "- [Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)](#section-2)\n",
    "- [Section 3: TwoLayerNet Architecture (2 points)](#section-3)\n",
    "- [Section 4: ConvNet Architecture (2 points)](#section-4)\n",
    "- [Section 5: Preparation of Training (7 points)](#section-5)\n",
    "- [Section 6: Training the Networks (5 points)](#section-6)\n",
    "- [Section 7: Setting Up the Hyperparameters (14 points)](#section-7)\n",
    "- [Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)](#section-8)\n",
    "- [Section 9: Fine-tuning ConvNet on STL-10 (14 points)](#section-9)\n",
    "- [Section 10: Bonus Challenge (optional)](#section-10)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t_qWNnrvtFp"
   },
   "source": [
    "### **Section 1: Image Classification on CIFAR-100 (0 points)**\n",
    "\n",
    "The goal of this lab is to implement an image classification system using Convolutional Neural Networks (CNNs) that can identify objects from a set of classes in the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). You will implement and compare two different architectures: a simple two-layer network and a ConvNet based on the LeNet architecture.\n",
    "\n",
    "The CIFAR-100 dataset contains 32x32 pixel RGB images, categorized into 100 different classes. The dataset will be automatically downloaded and loaded using the code provided in this notebook.\n",
    "\n",
    "You will train and test your classification system using the entire CIFAR-100 dataset. Ensure that the test images are excluded from training to maintain a fair evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:17:38.011035Z",
     "start_time": "2024-10-16T00:17:32.640197Z"
    },
    "id": "IO23QZYpvtFq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:17:38.044416Z",
     "start_time": "2024-10-16T00:17:38.014085Z"
    },
    "id": "m3Ew9c-W9cdj"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    '''\n",
    "    Set the seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed value.\n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:17:40.190233Z",
     "start_time": "2024-10-16T00:17:38.024647Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fjb8cUzsvtFr",
    "outputId": "136d2eea-6d6c-4521-f3a9-ae61ca3be8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaders for CIFAR-100 are ready for use.\n"
     ]
    }
   ],
   "source": [
    "# Define the transformations\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the CIFAR-100 test set\n",
    "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for the entire CIFAR-100 dataset\n",
    "train_data_loader = DataLoader(train_set, shuffle=True, num_workers=8)\n",
    "test_data_loader = DataLoader(test_set, shuffle=False, num_workers=8)\n",
    "\n",
    "print(\"Data loaders for CIFAR-100 are ready for use.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:17:40.215968Z",
     "start_time": "2024-10-16T00:17:40.192632Z"
    },
    "id": "1PHSl9TDvtFs"
   },
   "outputs": [],
   "source": [
    "# Define CIFAR-100 superclasses and their subclasses\n",
    "superclasses = {\n",
    "    'aquatic mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "    'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "    'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "    'food containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "    'fruit and vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "    'household electrical devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "    'household furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "    'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "    'large carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "    'large man-made outdoor things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "    'large natural outdoor scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "    'large omnivores and herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "    'medium-sized mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "    'non-insect invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "    'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "    'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "    'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "    'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "    'vehicles 1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "    'vehicles 2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "# List of all CIFAR-100 classes\n",
    "classes = ('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle',\n",
    "           'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "           'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "           'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "           'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "           'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "           'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
    "           'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower',\n",
    "           'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
    "           'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n",
    "\n",
    "# Create a mapping of class names to their indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "# Create a mapping of superclasses to their corresponding class indices\n",
    "superclass_to_indices = {supcls: [class_to_idx[cls] for cls in subclasses] for supcls, subclasses in superclasses.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aufKap2IvtFt"
   },
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)**\n",
    "\n",
    "In this section, you will implement a function to visualize the CIFAR-100 dataset, including **all** superclasses and their corresponding subclasses. Your implementation should provide a clear and organized overview of the dataset's diversity.\n",
    "\n",
    "You add the figure(s) to appendix of your report and refer to it in the main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:18:38.815584Z",
     "start_time": "2024-10-16T00:17:41.278635Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nxSovqo7vtFt",
    "outputId": "fce562b9-d9a1-4dab-cd94-9cc4aa5949da"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def visualise_cifar100(train_set, superclasses, superclass_to_indices, classes):\n",
    "    data = torch.tensor(train_set.data)\n",
    "    labels = torch.tensor(train_set.targets)\n",
    "\n",
    "    num_classes_per_superclass = len(superclasses['trees'])\n",
    "    num_samples = num_classes_per_superclass\n",
    "\n",
    "    for superclass in tqdm(superclasses.keys()):\n",
    "        fig, axs = plt.subplots(num_classes_per_superclass, num_samples, figsize=(25, 20))\n",
    "\n",
    "        # Set superclass name as plot title\n",
    "        fig.suptitle(f'Superclass: {superclass}', fontsize=20)\n",
    "        for num_class, index in enumerate(superclass_to_indices[superclass]):\n",
    "            subset = data[labels == index]\n",
    "            random_indices = torch.randperm(subset.size(0))\n",
    "        \n",
    "            # Set the class name as the label for the first column\n",
    "            class_name = classes[index]\n",
    "            axs[num_class, 0].set_ylabel(class_name, fontsize=16, rotation=0, labelpad=80, va='center')\n",
    "        \n",
    "            for num_sample, sample_index in enumerate(random_indices[ : num_samples]):\n",
    "                axs[num_class, num_sample].imshow(subset[sample_index])\n",
    "                axs[num_class, num_sample].set_xticks([])\n",
    "                axs[num_class, num_sample].set_yticks([])\n",
    "\n",
    "visualise_cifar100(train_set, superclasses, superclass_to_indices, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MMGIvvAvtFu"
   },
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: TwoLayerNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement the architecture of a fully connected neural network called `TwoLayerNet`, consisting of two fully connected layers with a ReLU activation in between. The network accepts an input size of 3x32x32 (CIFAR-100 image), a specified hidden layer size, and the number of output classes. In the `__init__` method, define the first fully connected layer that maps the input size to the hidden size, and the second fully connected layer that maps the hidden size to the number of classes.\n",
    "\n",
    "Ensure to call the parent class constructor using `super(TwoLayerNet, self).__init__()`. In the `forward` method, flatten the input tensor, pass it through the first layer with ReLU activation, and then through the second layer to obtain the final scores.\n",
    "\n",
    "**Note:** You are allowed to modify the provided function definitions as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:20:11.357430Z",
     "start_time": "2024-10-13T22:20:11.103725Z"
    },
    "id": "t33jJV2NvtFv"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes, extra_layers=False):\n",
    "        '''\n",
    "        Initializes the two-layer neural network model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input features.\n",
    "            hidden_size (int): The size of the hidden layer.\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super().__init__()\n",
    "\n",
    "        if extra_layers is False:\n",
    "            layers = [nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, num_classes)]\n",
    "\n",
    "        # MLP with 2 additional linear layers & a batch norm layer\n",
    "        else:\n",
    "            layers = [nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.BatchNorm1d(hidden_size), nn.ReLU(), nn.Linear(hidden_size, num_classes)]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # flatten input\n",
    "        x = x.flatten(start_dim=1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b4-vhb1vtFv"
   },
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: ConvNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement a convolutional neural network inspired by the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791). The network processes color images using three convolutional layers followed by two fully connected layers. Since you need to feed color images into this network, determine the kernel size of the first convolutional layer. Additionally, calculate the number of trainable parameters in the \"F6\" layer, providing the calculation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:00:11.771135Z",
     "start_time": "2024-10-14T08:00:11.725761Z"
    },
    "id": "WOmODnMCvtFw"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, non_linear=None, extra_layers=False, num_classes=100):\n",
    "        '''\n",
    "        Initializes the convolutional neural network model.\n",
    "        Follows the architecture of LeNet-5.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super().__init__()\n",
    "\n",
    "        # original activation functions from LeNet-5 paper\n",
    "        if non_linear is None:\n",
    "            non_linear1 = nn.Tanh()\n",
    "            non_linear2 = nn.Sigmoid()\n",
    "\n",
    "        # replace all activation functions with ReLU\n",
    "        elif non_linear == \"relu\":\n",
    "            non_linear1 = nn.ReLU()\n",
    "            non_linear2 = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid non-linear function. Please choose 'relu' or None.\")\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        if extra_layers is False:\n",
    "          # original architecture from LeNet-5\n",
    "\n",
    "          # Layer C1 is a convolutional layer with six feature maps.\n",
    "          # Each unit in each feature map is connected to a 5X5 neighborhood in the input.\n",
    "          # output: 6 feature maps of size 28X28\n",
    "          layers.append(nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5))\n",
    "          # each layer is passed through a squashing function (hyperbolic tangent)\n",
    "          layers.append(non_linear1)\n",
    "\n",
    "          # Layer S2 is a subsampling layer with six feature maps of size 14X14.\n",
    "          # Each unit in each feature map is connected to a 2X2 neighborhood in the corresponding feature map in C1.\n",
    "          # output: 6 feature maps of size 14X14\n",
    "          layers.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "          # The result is passed through a sigmoidal function.\n",
    "          layers.append(non_linear2)\n",
    "\n",
    "          # Layer C3 is a convolutional layer with 16 feature maps.\n",
    "          # Each unit in each feature map is connected to several 5X5\n",
    "          # neighborhoods at identical locations in a subset of S2’s feature maps\n",
    "          # output: 16 feature maps of size 10X10\n",
    "          layers.append(nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
    "          layers.append(non_linear1)\n",
    "\n",
    "          # Layer S4 is a subsampling layer with 16 feature maps of size 5X5.\n",
    "          # Each unit in each feature map is connected to a 2X2 neighborhood\n",
    "          # in the corresponding feature map in C3\n",
    "          layers.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "          # Layer C5 is a convolutional layer with 120 feature maps.\n",
    "          # Each unit is connected to a 5x5 neighborhood on all 16\n",
    "          # of S4’s feature maps.\n",
    "          layers.append(nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
    "          layers.append(non_linear1)\n",
    "\n",
    "          # Flatten the output of C5 (linear layer expects a 2D input of size (batch_size, num_features))\n",
    "          layers.append(nn.Flatten())\n",
    "\n",
    "          # Layer F6 contains 128 units and is fully connected to C5\n",
    "          # Layer F6 has 10 164 trainable parameters\n",
    "          # Total Parameters = input features ×output features + bias (of size output features)\n",
    "          # 120 * 84 + 84 = 10164\n",
    "          layers.append(nn.Linear(in_features=120, out_features=84))\n",
    "          layers.append(non_linear1)\n",
    "\n",
    "          layers.append(nn.Linear(in_features=84, out_features=num_classes))\n",
    "\n",
    "        else:\n",
    "          # additional batch norm, max pool, and dropouts applied (not counted as extra layers)\n",
    "          # convolutional channel sizes adjusted\n",
    "          layers.append(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5))\n",
    "          layers.append(nn.BatchNorm2d(64))\n",
    "          layers.append(nn.ReLU())\n",
    "          layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "          layers.append(nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3))\n",
    "          layers.append(nn.BatchNorm2d(96))\n",
    "          layers.append(nn.ReLU())\n",
    "          layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "          layers.append(nn.Conv2d(in_channels=96, out_channels=128, kernel_size=3))\n",
    "          layers.append(nn.BatchNorm2d(128))\n",
    "          layers.append(nn.ReLU())\n",
    "\n",
    "          # 1st addtional layer - convolutional layer\n",
    "          layers.append(nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3))\n",
    "          layers.append(nn.BatchNorm2d(128))\n",
    "          layers.append(nn.ReLU())\n",
    "\n",
    "          layers.append(nn.Flatten(start_dim=1))\n",
    "\n",
    "          # 2nd addtional layer - linear, in_features reshaped to match flatten\n",
    "          layers.append(nn.Linear(in_features=512, out_features=256))\n",
    "          layers.append(nn.ReLU())\n",
    "\n",
    "          layers.append(nn.Dropout(p=0.5))\n",
    "\n",
    "          layers.append(nn.Linear(in_features=256, out_features=128))\n",
    "          layers.append(nn.ReLU())\n",
    "\n",
    "          layers.append(nn.Linear(in_features=128, out_features=num_classes))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        out = self.model(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds2XBCiFvtFw"
   },
   "source": [
    "<a id=\"section-5\"></a>\n",
    "### **Section 5: Preparation of Training (7 points)**\n",
    "\n",
    "In this section, you will create a custom dataset class to load the CIFAR-100 data, define a transform function for data augmentation, and set up an optimizer for training. While the previous section utilized the built-in CIFAR-100 class from `torchvision`, in practice, you often need to prepare datasets manually. Here, you will implement the `CIFAR100_loader` class to handle the dataset and use `DataLoader` to make it iterable. You will also define a transform function for data augmentation and an optimizer for updating the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:20:18.545082Z",
     "start_time": "2024-10-13T22:20:18.507863Z"
    },
    "id": "I9WhAe44vtFx"
   },
   "outputs": [],
   "source": [
    "class CIFAR100_loader(Dataset):\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, download=False):\n",
    "        '''\n",
    "        Initializes the CIFAR-100 dataset loader.\n",
    "\n",
    "        Args:\n",
    "            root (str): The root directory to store the dataset.\n",
    "            train (bool): If True, loads the training data; otherwise, loads the test data.\n",
    "            transform (callable, optional): The data transformations to apply.\n",
    "            download (bool): If True, downloads the dataset if it is not already available.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.data = torchvision.datasets.CIFAR100(root=root, train=train, download=download, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and label tensors.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pickle\n",
    "# import requests\n",
    "# import tarfile\n",
    "# import os\n",
    "\n",
    "# class CIFAR100_loader(Dataset):\n",
    "    \n",
    "#     def __init__(self, root, train=True, transform=None, download=False, validate=False):\n",
    "#         self.root = root\n",
    "#         self.train = train\n",
    "#         self.transform = transform\n",
    "\n",
    "#         CWD = Path(os.getcwd())\n",
    "#         ROOT = CWD/Path(self.root)\n",
    "#         os.makedirs(ROOT, exist_ok=True)\n",
    "        \n",
    "#         if download:\n",
    "#             os.chdir(ROOT)\n",
    "#             url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "#             file = \"cifar-100-python.tar.gz\"\n",
    "#             req = requests.get(url, stream=True)\n",
    "#             open(file, \"wb\").write(req.content)\n",
    "#             tarfile.open(file, \"r:gz\").extractall(filter=\"fully_trusted\")\n",
    "#             os.chdir(CWD)\n",
    "\n",
    "#         with open(ROOT/Path(\"cifar-100-python/meta\"), \"rb\") as f:\n",
    "#             meta = pickle.load(f)\n",
    "#             self.all_labels = tuple(meta[\"fine_label_names\"])\n",
    "\n",
    "#         with open(f\"./data/cifar-100-python/{\"train\" if train else \"test\"}\", \"rb\") as f:\n",
    "#             dataset = pickle.load(f, encoding=\"bytes\")\n",
    "#             self.labels = dataset[b'fine_labels']\n",
    "#             self.images = []\n",
    "#             for i in dataset[b'data']:\n",
    "#                 img = np.zeros((32,32,3))\n",
    "#                 for j in range(3):\n",
    "#                     img[:,:,j] = np.array(i[j*1024 : (j+1)*1024]).reshape((32,32))\n",
    "#                 self.images.append(Image.fromarray(np.uint8(img)))\n",
    "\n",
    "#         if validate:\n",
    "#             self.images = self.images[:6969]\n",
    "#             self.labels = self.labels[:6969]\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         if self.transform:\n",
    "#             return self.transform(self.images[idx]), self.labels[idx]\n",
    "#         else:\n",
    "#             to_tensor = transforms.ToTensor()\n",
    "#             return to_tensor(self.images[idx]), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:25:29.970583Z",
     "start_time": "2024-10-13T22:25:29.932620Z"
    },
    "id": "moTeDvS7vtFx"
   },
   "outputs": [],
   "source": [
    "def create_transforms(additional_transform=False):\n",
    "    '''\n",
    "    Creates the data transformations for the CIFAR-100 dataset.\n",
    "    Args:\n",
    "        additional_transform (bool): If True, apply additional transformations.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: The data transformations for the dataset.\n",
    "    '''\n",
    "\n",
    "    if additional_transform is False:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    # additional transformations for hyperparameter tuning\n",
    "    elif additional_transform is True:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid transformation type. Please choose 'original' or 'augmented'.\")\n",
    "\n",
    "    return transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:25:36.719096Z",
     "start_time": "2024-10-13T22:25:36.675384Z"
    },
    "id": "6lO_9TSnvtFx"
   },
   "outputs": [],
   "source": [
    "def create_optimizer(model, learning_rate=0.001, weight_decay=1.0, optimizer='adam'):\n",
    "    '''\n",
    "    Creates an optimizer for the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        weight_decay (float): The L2 regularization strength.\n",
    "        optimizer (str): The optimizer to use.\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Adam: The optimizer for the model.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Adam optimiser with L2 regularisation\n",
    "    if optimizer == \"adam\":\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "\n",
    "    # additional optimizers for hyperparameter tuning\n",
    "    elif optimizer == \"sgd\":\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        opt = torch.optim.RMSprop(model.parameters(), lr=1e-3, alpha=0.99, eps=1e-8, weight_decay=1e-5)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer. Please choose 'adam', 'sgd', or 'rmsprop'\")\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFISOKDOvtFx"
   },
   "source": [
    "<a id=\"section-6\"></a>\n",
    "### **Section 6: Training the Networks (5 points)**\n",
    "\n",
    "In this section, you will complete the `train` function and use it to train both the `TwoLayerNet` and `ConvNet` models. You will use the custom `CIFAR100_loader`, transform function, and optimizer function that you implemented. The goal is to compare the performance of the two models on the CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:25:42.066597Z",
     "start_time": "2024-10-13T22:25:42.031330Z"
    },
    "id": "0rrY-WlBvtFy"
   },
   "outputs": [],
   "source": [
    "def validate(net, testloader):\n",
    "    '''\n",
    "    Validates the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        test_results (dict): Dictionary with the accuracy, precision, recall, and f1 scores for the model.\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = net.to(device)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    test_results = {}\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in testloader:\n",
    "\n",
    "            # move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # pass inputs to model\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # get predicted class\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # sum correct predictions & total predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "\n",
    "    # calculate metrics\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred, average='weighted', zero_division=0.0)\n",
    "    recall = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    test_results['accuracy'] = accuracy\n",
    "    test_results['precision'] = precision\n",
    "    test_results['recall'] = recall\n",
    "    test_results['f1'] = f1\n",
    "\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:25:51.154688Z",
     "start_time": "2024-10-13T22:25:51.118495Z"
    },
    "id": "dC8Ha1D9vtFy"
   },
   "outputs": [],
   "source": [
    "def validate_per_class(net, testloader, classes):\n",
    "    '''\n",
    "    Validates the model on the test dataset per class.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "        classes (tuple): The tuple of class names.\n",
    "\n",
    "    Returns:\n",
    "        class_accuracy (dictionary): The accuracy per class for the model.\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = net.to(device)\n",
    "\n",
    "    class_correct = [0. for _ in range(len(classes))]\n",
    "    class_total = [0. for _ in range(len(classes))]\n",
    "\n",
    "    class_accuracy = {}\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in tqdm(testloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions = (predicted == labels).squeeze()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += correct_predictions[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "        class_accuracy[class_name] = accuracy\n",
    "\n",
    "    return class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:25:55.043680Z",
     "start_time": "2024-10-13T22:25:55.007697Z"
    },
    "id": "WxmipsKMvtFy"
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, criterion, optimizer, epochs=100):\n",
    "    '''\n",
    "    Trains the neural network model.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader for the training dataset.\n",
    "        test_loader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        epochs (int): The number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        epoch_performances (list): Performance of the model for each epoch. For plotting.\n",
    "        epoch_losses (list): Loss for each epoch. For plotting.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Set the model to train mode\n",
    "    net.train()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = net.to(device)\n",
    "\n",
    "    epoch_performances = []\n",
    "    epoch_performances_train = []\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0.0\n",
    "        # Iterate over the train dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_loader) # average over the batch\n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "        epoch_performances.append(validate(net, test_loader))\n",
    "        epoch_performances_train.append(validate(net, train_loader))\n",
    "\n",
    "        if type(train_loader)==list:\n",
    "            random.shuffle(train_loader)\n",
    "\n",
    "    return epoch_performances, epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBqErvFgDnNk"
   },
   "outputs": [],
   "source": [
    "def plot_epoch_performances(epoch_performances, model_name, metric=\"accuracy\"):\n",
    "    '''\n",
    "    Plots the performance of the model for each epoch.\n",
    "\n",
    "    Args:\n",
    "        epoch_performances (list): List of dictionaries of the performance of the model for each epoch (accuracy, precision, recall, f1).\n",
    "        metric (str): The metric to plot.\n",
    "    '''\n",
    "    print(\"metric: \", metric)\n",
    "    # list of values from 0 to number of epochs\n",
    "    epoch = [i for i in range(len(epoch_performances))]\n",
    "    print(\"epoch length: \", len(epoch_performances))\n",
    "\n",
    "    if metric == \"loss\":\n",
    "      print(\"hello world\")\n",
    "      y = epoch_performances\n",
    "\n",
    "    else:\n",
    "      # get list of metric values for each epoch from ditionary\n",
    "      y = [epoch_performances[i][metric] for i in range(len(epoch_performances))]\n",
    "\n",
    "    plt.title(f\"Test {metric} over training epochs for model {model_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(f\"{metric}\")\n",
    "    plt.plot(epoch, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "m_ti6C5kvtFz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, initialize the datasets and data loaders for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:26:02.723046Z",
     "start_time": "2024-10-13T22:26:02.680909Z"
    },
    "id": "9yKtf1aNvtFz"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def create_dataloaders(additional_transform=True, batch_size=512, use_cache=False, dataset='CIFAR100'):\n",
    "    '''\n",
    "    Create the data loaders for the CIFAR 100 dataset\n",
    "\n",
    "    Args:\n",
    "      additional_transform (bool): whether to use additional transformations\n",
    "      batch_size (int): the batch size for the data loaders\n",
    "      use_cache (bool): whether to use the cache for speeding up process\n",
    "\n",
    "    Returns:\n",
    "        train_data_loader (torch.utils.data.DataLoader): Data loader for the training dataset\n",
    "        test_data_loader (torch.utils.data.DataLoader): Data loader for the test dataset\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Define the transformations\n",
    "    transform = create_transforms(additional_transform=additional_transform)\n",
    "\n",
    "    # Load the training set\n",
    "    if dataset == 'CIFAR100':\n",
    "        train_set = CIFAR100_loader(root='./data', train=True, download=True, transform=transform)\n",
    "    elif dataset == 'STL10':\n",
    "        train_set = STL10_loader(root='./data', train=True, download=True, transform=transform)\n",
    "    else:\n",
    "        print('Invalid dataset choice')\n",
    "\n",
    "    # Load the test set\n",
    "    if dataset == 'CIFAR100':\n",
    "        test_set = CIFAR100_loader(root='./data', train=False, download=True, transform=transform)\n",
    "    elif dataset == 'STL10':\n",
    "        test_set = STL10_loader(root='./data', train=False, download=True, transform=transform)\n",
    "    else:\n",
    "        print('Invalid dataset choice')\n",
    "\n",
    "    # Create data loaders for the entire CIFAR-100 dataset\n",
    "    train_data_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers = 2, persistent_workers=True)\n",
    "    test_data_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers = 2, persistent_workers=True)\n",
    "\n",
    "    # for speeding up the data loading\n",
    "    if use_cache:\n",
    "      cache_train = []\n",
    "      print (\"warming up train loader\")\n",
    "      for idx, data in enumerate(tqdm(train_data_loader)):\n",
    "          cache_train.append(data)\n",
    "\n",
    "      cache_test = []\n",
    "      print (\"warming up test loader\")\n",
    "      for idx, data in enumerate(tqdm(test_data_loader)):\n",
    "          cache_test.append(data)\n",
    "      return cache_train, cache_test\n",
    "\n",
    "    else:\n",
    "      return train_data_loader, test_data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xqxyGEVdvtFz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, train the TwoLayerNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3dvDhWavtFz",
    "outputId": "34b54cec-da24-405c-88e3-0a04b151316f"
   },
   "outputs": [],
   "source": [
    "train_data_loader, test_data_loader = create_dataloaders(use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-12T13:23:05.268062Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l0lIsWZVvtFz",
    "outputId": "b2c5f0de-00fd-404f-bf4d-c931cec6679b"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "epochs = 100\n",
    "learning_rate=0.001\n",
    "\n",
    "mlp = TwoLayerNet(32 * 32 * 3, 1024, 100)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_mlp = create_optimizer(mlp, learning_rate)\n",
    "\n",
    "mlp_epoch_performances, mlp_epoch_losses = train(mlp, train_data_loader, test_data_loader, criterion, optimizer_mlp, epochs)\n",
    "plot_epoch_performances(mlp_epoch_performances, model_name=\"MLP\")\n",
    "plot_epoch_performances(mlp_epoch_losses, model_name=\"MLP\", metric=\"loss\")\n",
    "\n",
    "test_mlp = validate(mlp, test_data_loader)\n",
    "print(\"MLP test accuracy: \", test_mlp)\n",
    "\n",
    "test_mlp_per_class = validate_per_class(mlp, test_data_loader, classes)\n",
    "print(\"Test Accuracy MLP per class: \", test_mlp_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NDtTXa5ZvtF0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Finally, train the ConvNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:12:20.387042Z",
     "start_time": "2024-10-13T16:51:05.690681Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LViQyvlWvtF0",
    "outputId": "537f7c10-48ae-4024-b7c4-99b644f72d44"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "epochs = 100\n",
    "learning_rate=0.001\n",
    "\n",
    "conv_net = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = create_optimizer(conv_net, learning_rate)\n",
    "\n",
    "cnn_epoch_performances, cnn_epoch_losses = train(conv_net, train_data_loader, test_data_loader, criterion, optimizer_cnn, epochs)\n",
    "plot_epoch_performances(cnn_epoch_performances, model_name=\"CNN\")\n",
    "plot_epoch_performances(cnn_epoch_losses, model_name=\"CNN\", metric=\"loss\")\n",
    "\n",
    "test_accuracy_cnn = validate(conv_net, test_data_loader)\n",
    "test_accuracy_cnn_per_class = validate_per_class(conv_net, test_data_loader, classes)\n",
    "\n",
    "print(\"CNN test accuracy: \", test_accuracy_cnn)\n",
    "print(\"Test Accuracy CNN per class: \", test_accuracy_cnn_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sGN_OrlvvtF0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a id=\"section-7\"></a>\n",
    "### **Section 7: Setting Up the Hyperparameters (14 points)**\n",
    "\n",
    "In this section, you will experiment with both the `ConvNet` and `TwoLayerNet` models by setting up and tuning the hyperparameters to achieve the highest possible accuracy. You have the flexibility to modify the training process, including the `train` function, `DataLoader`, `transform` functions, and optimizer as needed.\n",
    "\n",
    "1. Adjust the hyperparameters, including learning rate, batch size, number of epochs, optimizer, weight decay, and transform function to improve the performance of both networks. Modify the training procedure and architecture as necessary. You can also add components like Batch Normalization layers.\n",
    "2. Add two more layers to both `TwoLayerNet` and `ConvNet`. You can decide the size and placement of these layers. Evaluate if these changes result in higher performance and explain your findings.\n",
    "3. Show the final results and describe the modifications made to enhance performance. Discuss the impact of hyperparameter tuning on both `TwoLayerNet` and `ConvNet`.\n",
    "4. Compare the two networks in terms of architecture, performance, and learning rates. Provide a detailed explanation of the differences observed.\n",
    "\n",
    "**Note:** Do not use external pre-trained networks and limit additional convolutional layers to a maximum of three beyond the original architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9hhrKQRvtF0"
   },
   "source": [
    "Test the performance of TwoLayerNet after hyperparameter tuning and compare it with the ConvNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:21:38.037227Z",
     "start_time": "2024-10-14T00:21:38.022264Z"
    },
    "id": "vhhyy-CRvtF0"
   },
   "outputs": [],
   "source": [
    "def sweep(train_data_loader, test_data_loader, learning_rate=0.001, batch_size=512, epochs=100, optimizer='adam', weight_decay=1.0, nonlinear_fn=None, transformations=False, model='cnn', chkpt=None, extra_layers=False):\n",
    "    '''\n",
    "    Sweeps hyperparameters for hyperparameter tuning\n",
    "\n",
    "    Args:\n",
    "        train_data_loader (torch.utils.data.DataLoader): The data loader for the training dataset.\n",
    "        test_data_loader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "        learning_rate (float): The learning rate to test.\n",
    "        batch_size (int): The batch size to test.\n",
    "        epochs (int): The number of epochs to test.\n",
    "        optimizer (str): The optimizer to test.\n",
    "        weight_decay (float): The weight decay to test.\n",
    "        nonlinear_fn (str): The non-linear function to test for the CNN model.\n",
    "        transformations (bool): The data transformations to test.\n",
    "        model (str): The model (CNN or MLP).\n",
    "        chkpt (torch.nn.Module): Checkpoint of the model if it already exists for fast loading.\n",
    "        extra_layers (bool): For testing architecture of the model with additional layers.\n",
    "\n",
    "    Returns:\n",
    "        results (dict): A dict of 2 dictionaries containing the test metrics and accuracy per class\n",
    "        model (torch.nn.Module): The model to save.\n",
    "    '''\n",
    "\n",
    "    # original hyperparameters\n",
    "    # lr = 0.001, batch_size = 512, epochs = 100, optimizer = AdamW, weight_decay = 1, nonlinear_fn = sigmoid/tanh, layers = original, transformations = original\n",
    "\n",
    "    # if the model checkpoint doesn't exist instantiate the network & train as usual\n",
    "    if chkpt is None:\n",
    "        if model == \"mlp\":\n",
    "            net = TwoLayerNet(input_size=32 * 32 * 3, hidden_size=1024, num_classes=100, extra_layers=extra_layers)\n",
    "        elif model == \"cnn\":\n",
    "            net = ConvNet(non_linear=nonlinear_fn, extra_layers=extra_layers)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model. Please choose 'mlp' or 'cnn'\")\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        opt = create_optimizer(net, learning_rate=learning_rate, weight_decay=weight_decay, optimizer=optimizer)\n",
    "        train(net=net, train_loader=train_data_loader, test_loader=test_data_loader, criterion=criterion, optimizer=opt, epochs=epochs)\n",
    "\n",
    "    # if model checkpoint exists, use checkpoing\n",
    "    elif chkpt is not None:\n",
    "        net = chkpt\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid checkpoint. Please provide a valid checkpoint.\")\n",
    "\n",
    "    # validate test accuracy for the model & per class\n",
    "    test_accuracy = validate(net, test_data_loader)\n",
    "    test_accuracy_per_class = validate_per_class(net, test_data_loader, classes)\n",
    "\n",
    "    results = {\"metrics\": test_accuracy, \"class accuracies\": test_accuracy_per_class}\n",
    "\n",
    "    return results, net\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:43:48.015519Z",
     "start_time": "2024-10-13T22:43:47.972859Z"
    },
    "id": "0UN4irgzvtF2"
   },
   "outputs": [],
   "source": [
    "def read_sweep_results(results_file):\n",
    "\n",
    "    \"\"\"\n",
    "    Read the sweep results from the JSON file in case of a crash or for easy loading.\n",
    "\n",
    "    Args:\n",
    "        results_file (str): Path to the JSON file containing the results.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the hyperparameter sweep results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:51:12.745666Z",
     "start_time": "2024-10-14T00:51:12.588074Z"
    },
    "id": "i6bMGjoyvtF7"
   },
   "outputs": [],
   "source": [
    "def run_sweep(train_data_loader, test_data_loader, hyperparameter_name, hyperparameter_values, sweep_results, m, save_path=\"model_chkpts\", results_file=\"sweep_results.json\"):\n",
    "    \"\"\"\n",
    "    Run sweeps for different hyperparameters, save model checkpoints, and write results to disk.\n",
    "\n",
    "    Args:\n",
    "        train_data_loader (torch.utils.data.DataLoader): The data loader for the training dataset.\n",
    "        test_data_loader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "        hyperparameter_name (str): Name of the hyperparameter to be tested (e.g. 'learning_rate', 'batch_size').\n",
    "        hyperparameter_values (list): A list of test values for the hyperparameter.\n",
    "        sweep_results (dict): Global dictionary for storing sweep results.\n",
    "        m (str): Model name ('mlp' or 'cnn').\n",
    "        save_path (str): Directory path where model checkpoints will be saved.\n",
    "        results_file (str): Path to the JSON file where sweep results will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure the save path directory actually exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    if hyperparameter_name != \"extra_layers\":\n",
    "        extra_layer = False\n",
    "    elif hyperparameter_name == \"extra_layers\":\n",
    "        extra_layer = hyperparameter_values[0]\n",
    "\n",
    "    for value in hyperparameter_values:\n",
    "        print(\"Running sweep for: \", hyperparameter_name, value)\n",
    "\n",
    "        # Define the model filename based on hyperparameter value\n",
    "        model_filename = f\"{m}_model_{hyperparameter_name}_{value}.pt\"\n",
    "        model_filepath = os.path.join(save_path, model_filename)\n",
    "\n",
    "        # Check if the model checkpoint already exists\n",
    "        if os.path.exists(model_filepath):\n",
    "            print(f\"Loading existing model from: {model_filepath}\")\n",
    "\n",
    "            # instantiate a network\n",
    "            if m == \"cnn\":\n",
    "                model = ConvNet(extra_layers=extra_layer)\n",
    "            elif m == \"mlp\":\n",
    "                model = TwoLayerNet(32 * 32 * 3, 1024, 100, extra_layers=extra_layer)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model. Please choose 'mlp' or 'cnn'\")\n",
    "\n",
    "            # load the checkpoint weights onto the model\n",
    "            model.load_state_dict(torch.load(model_filepath, weights_only=True))\n",
    "\n",
    "            # pass hyperparameter testing values to the sweep function\n",
    "            kwargs = {'train_data_loader': train_data_loader, 'test_data_loader': test_data_loader, hyperparameter_name: value, 'model': m, 'chkpt': model}\n",
    "            results, model = sweep(**kwargs)\n",
    "\n",
    "            if hyperparameter_name not in sweep_results:\n",
    "                sweep_results[hyperparameter_name] = {}\n",
    "\n",
    "            sweep_results[hyperparameter_name][value] = results\n",
    "\n",
    "            print(\"Results retrieved for: \", hyperparameter_name, value)\n",
    "\n",
    "        else:\n",
    "            # run the sweep for the current hyperparameter value\n",
    "            # ex. sweep(learning_rate=0.01)\n",
    "            kwargs = {'train_data_loader': train_data_loader, 'test_data_loader': test_data_loader, hyperparameter_name: value, 'model': m}\n",
    "            results, model = sweep(**kwargs)\n",
    "\n",
    "            # store the results for the current hyperparameter value\n",
    "            if hyperparameter_name not in sweep_results:\n",
    "                sweep_results[hyperparameter_name] = {}\n",
    "            sweep_results[hyperparameter_name][value] = results\n",
    "\n",
    "            # save the model checkpoint in case of a crash\n",
    "            torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "            # save the sweep results to a JSON file in case of a crash\n",
    "            with open(f\"{m}_{results_file}\", \"w\") as f:\n",
    "                json.dump(sweep_results, f)\n",
    "\n",
    "            print(\"Results saved for: \", hyperparameter_name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "HDydoSL1vtF1",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Test the performance of ConvNet after hyperparameter tuning and compare it with the TwoLayerNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:21:44.722826Z",
     "start_time": "2024-10-14T00:21:44.678172Z"
    },
    "id": "1O5sf2v6vtF1"
   },
   "outputs": [],
   "source": [
    "cnn_sweep_results = {'original': {}, 'learning_rate': {}, 'batch_size': {}, 'epochs': {}, 'optimizer': {}, 'weight_decay': {}, 'nonlinear_fn': {}, 'layers': {}, 'transformations': {}}\n",
    "mlp_sweep_results = {'original': {}, 'learning_rate': {}, 'batch_size': {}, 'epochs': {}, 'optimizer': {}, 'weight_decay': {}, 'nonlinear_fn': {}, 'layers': {}, 'transformations': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrG1ZYdhBbeN",
    "outputId": "a5666fdd-5638-48fd-9734-f00c171d8307"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# create train & test data loaders once\n",
    "train_data_loader, test_data_loader = create_dataloaders(use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2R_gs0p0H0E",
    "outputId": "df14a85c-0623-4d64-aa81-5ca645d6a4e6"
   },
   "outputs": [],
   "source": [
    "# mlp baseline vals with original hyperparams\n",
    "results, model = sweep(train_data_loader, test_data_loader)\n",
    "os.makedirs(\"model_chkpts/\", exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"model_chkpts/mlp_model_original.pt\")\n",
    "mlp_sweep_results['original'] = results\n",
    "\n",
    "with open(\"mlp_sweep_results.json\", \"w\") as f:\n",
    "    json.dump(mlp_sweep_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:45:05.888271Z",
     "start_time": "2024-10-14T00:21:46.035430Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYNoumbQvtF1",
    "outputId": "824ddaaf-b0b6-48ba-941a-9305c18192fa"
   },
   "outputs": [],
   "source": [
    "# cnn baseline vals with original hyperparams\n",
    "results, model = sweep(train_data_loader, test_data_loader)\n",
    "torch.save(model.state_dict(), f\"model_chkpts/cnn_model_original.pt\")\n",
    "cnn_sweep_results['original'] = results\n",
    "\n",
    "with open(\"cnn_sweep_results.json\", \"w\") as f:\n",
    "    json.dump(cnn_sweep_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOh2RbtaNqrW"
   },
   "outputs": [],
   "source": [
    "# lists of hyperparameters for tuning\n",
    "cnn_hyperparam_names = [\"learning_rate\", \"batch_size\", \"epochs\", \"optimizer\", \"weight_decay\", \"nonlinear_fn\", \"transformations\", \"extra_layers\"]\n",
    "mlp_hyperparam_names = [\"learning_rate\", \"batch_size\", \"epochs\", \"optimizer\", \"weight_decay\", \"transformations\", \"extra_layers\"]\n",
    "\n",
    "learning_rates = [0.0001, 0.01]\n",
    "batch_sizes = [32, 64, 128, 256, 1024]\n",
    "epochs = [10, 50, 200, 500]\n",
    "optimizers = ['sgd', 'rmsprop']\n",
    "weight_decays = [0.8, 0.6, 0.4, 0.2, 0.1, 0.01]\n",
    "nonlinear_fns = [\"relu\"]\n",
    "transformations = [True]\n",
    "layers = [True]\n",
    "\n",
    "cnn_hyperparams = [learning_rates, batch_sizes, epochs, optimizers, weight_decays, nonlinear_fns, transformations, layers]\n",
    "mlp_hyperparams = [learning_rates, batch_sizes, epochs, optimizers, weight_decays, transformations, layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHk5hj184Ans",
    "outputId": "ca67d70d-38ba-47f3-aba5-4d48018a8411"
   },
   "outputs": [],
   "source": [
    "# Tune MLP\n",
    "m = \"mlp\"\n",
    "\n",
    "# Run sweeps\n",
    "for i in tqdm(range(len(mlp_hyperparam_names))):\n",
    "    run_sweep(train_data_loader, test_data_loader, mlp_hyperparam_names[i], mlp_hyperparams[i], mlp_sweep_results, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:46:06.485439Z",
     "start_time": "2024-10-14T00:51:45.728966Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnHdXxOyvtF8",
    "outputId": "91a08a7f-639f-4d51-ba80-aa8019ab73d8"
   },
   "outputs": [],
   "source": [
    "# Tune CNN\n",
    "m = \"cnn\"\n",
    "\n",
    "# Run sweeps\n",
    "for i in tqdm(range(len(cnn_hyperparam_names))):\n",
    "    run_sweep(train_data_loader, test_data_loader, cnn_hyperparam_names[i], cnn_hyperparams[i], cnn_sweep_results, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiYV8b2E6QMH",
    "outputId": "c5857ded-b680-4343-b7e2-71804b303061"
   },
   "outputs": [],
   "source": [
    "def json_to_dict(json_file):\n",
    "    \"\"\"\n",
    "    Loads a JSON file to dictionary (to use in case of crash or sweep dictionary is lost from memory)\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The contents of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data\n",
    "\n",
    "cnn_sweep_results = json_to_dict(\"cnn_sweep_results.json\")\n",
    "mlp_sweep_results = json_to_dict(\"mlp_sweep_results.json\")\n",
    "print(cnn_sweep_results)\n",
    "print(mlp_sweep_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b6MKJ6ds5LWV",
    "outputId": "e82fe0d9-f95a-4286-f292-a9c746b0d287"
   },
   "outputs": [],
   "source": [
    "# Analyze CNN tuning\n",
    "\n",
    "def plot_hyperparameter_comparison(sweep_results, model, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Plot the hyperparameter tuning vs baseline metrics\n",
    "\n",
    "    Args:\n",
    "        sweep_results (dict): Dictionary containing the sweep results.\n",
    "        model (str): The model name (e.g., \"mlp\", \"cnn\").\n",
    "        metric (str): The metric to compare (e.g., \"accuracy\", \"precision\", etc.).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Extract baseline metrics\n",
    "    baseline_metrics = sweep_results[\"original\"][\"metrics\"]\n",
    "    baseline_value = baseline_metrics.get(metric, None)\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    hyperparameters = []\n",
    "    values = []\n",
    "    baseline = []\n",
    "\n",
    "    for hyperparam, results in sweep_results.items():\n",
    "        # skip baseline\n",
    "        if hyperparam == \"original\":\n",
    "            continue\n",
    "\n",
    "        for param_value, result in results.items():\n",
    "            hyperparameters.append(f\"{hyperparam}={param_value}\")\n",
    "            values.append(result[\"metrics\"].get(metric, None))\n",
    "            baseline.append(baseline_value)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(hyperparameters, values, label=\"Tuned results\", marker=\"o\", color=\"blue\")\n",
    "    plt.plot(hyperparameters, baseline, label=\"Baseline\", linestyle=\"--\", marker=\"x\", color=\"red\")\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Hyperparameters\")\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.title(f\"Comparison of {metric.capitalize()} across Hyperparameters for {model}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_hyperparameter_comparison(cnn_sweep_results, model=\"cnn\", metric=\"accuracy\")\n",
    "# plot_hyperparameter_comparison(cnn_sweep_results, model=\"cnn\", metric=\"precision\")\n",
    "# plot_hyperparameter_comparison(cnn_sweep_results, model=\"cnn\", metric=\"recall\")\n",
    "# plot_hyperparameter_comparison(cnn_sweep_results, model=\"cnn\", metric=\"f1\")\n",
    "\n",
    "plot_hyperparameter_comparison(mlp_sweep_results, model=\"mlp\", metric=\"accuracy\")\n",
    "# plot_hyperparameter_comparison(mlp_sweep_results, model=\"mlp\", metric=\"precision\")\n",
    "# plot_hyperparameter_comparison(mlp_sweep_results, model=\"mlp\", metric=\"recall\")\n",
    "# plot_hyperparameter_comparison(mlp_sweep_results, model=\"mlp\", metric=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gRAC1JtAvtF8",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Add two more layers to both `TwoLayerNet` and `ConvNet`. You can decide the size and placement of these layers. Evaluate if these changes result in higher performance and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XJZh3XIdttB",
    "outputId": "d0234086-fe27-4d3d-a9b2-23411a86a605"
   },
   "outputs": [],
   "source": [
    "# Tuned model MLP performance\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "extra_layers = True\n",
    "transformations = True\n",
    "weight_decay = 0.8\n",
    "\n",
    "train_data_loader_mlp, test_data_loader_mlp = create_dataloaders(additional_transform=transformations, batch_size=batch_size, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9y2oFwTM3PtN",
    "outputId": "114d8711-3ff9-4e7c-dbe5-8cda6e5f6f48"
   },
   "outputs": [],
   "source": [
    "mlp = TwoLayerNet(32 * 32 * 3, 1024, 100, extra_layers=extra_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_mlp = create_optimizer(mlp, learning_rate, weight_decay)\n",
    "\n",
    "mlp_epoch_performances, mlp_epoch_losses = train(mlp, train_data_loader_mlp, test_data_loader_mlp, criterion, optimizer_mlp, epochs)\n",
    "plot_epoch_performances(mlp_epoch_performances, model_name=\"MLP\")\n",
    "plot_epoch_performances(mlp_epoch_losses, model_name=\"MLP\", metric=\"loss\")\n",
    "\n",
    "test_mlp = validate(mlp, test_data_loader_mlp)\n",
    "print(\"MLP test accuracy: \", test_mlp)\n",
    "\n",
    "test_mlp_per_class = validate_per_class(mlp, test_data_loader_mlp, classes)\n",
    "print(\"Test Accuracy MLP per class: \", test_mlp_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0EPRd_ixvtF8",
    "outputId": "18a39aff-44fc-4a2b-db48-de2e560cc0d1"
   },
   "outputs": [],
   "source": [
    "# Tuned model CNN performance\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "batch_size = 512\n",
    "extra_layers = True\n",
    "transformations = True\n",
    "weight_decay = 0.8\n",
    "nonlinear_fn = \"relu\"\n",
    "\n",
    "train_data_loader_cnn, test_data_loader_cnn = create_dataloaders(additional_transform=transformations, batch_size=batch_size, use_cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-1prx5lyuR-B",
    "outputId": "919689c8-ca3b-43bf-ea3e-f60600d403dc"
   },
   "outputs": [],
   "source": [
    "conv_net = ConvNet(extra_layers=extra_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = create_optimizer(conv_net, learning_rate, weight_decay)\n",
    "\n",
    "cnn_epoch_performances, cnn_epoch_losses = train(conv_net, train_data_loader_cnn, test_data_loader_cnn, criterion, optimizer_cnn, epochs)\n",
    "plot_epoch_performances(cnn_epoch_performances, model_name=\"CNN\")\n",
    "plot_epoch_performances(cnn_epoch_losses, model_name=\"CNN\", metric=\"loss\")\n",
    "\n",
    "test_accuracy_cnn = validate(conv_net, test_data_loader_cnn)\n",
    "print(\"CNN test accuracy: \", test_accuracy_cnn)\n",
    "\n",
    "test_accuracy_cnn_per_class = validate_per_class(conv_net, test_data_loader_cnn, classes)\n",
    "print(\"Test Accuracy CNN per class: \", test_accuracy_cnn_per_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKPG1L96xh86"
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_per_class(model_name, accuracy_per_class):\n",
    "  \"\"\"\n",
    "    Plot the top 5 and bottom 5 accuracies\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model.\n",
    "        accuracy_per_class (dict): Dictionary containing class names and accuracies.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "  \"\"\"\n",
    "  # Extract class names and accuracies\n",
    "  sorted_accuracy = sorted(accuracy_per_class.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Get the bottom 5 and top 5 accuracies\n",
    "  bottom_5 = sorted_accuracy[-5:]\n",
    "  top_5 = sorted_accuracy[:5]\n",
    "\n",
    "  # Combine them for plotting\n",
    "  combined = top_5 + bottom_5\n",
    "  classes, accuracies = zip(*combined)  # Unzip into two lists\n",
    "\n",
    "  # Plotting\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.bar(classes, accuracies, color=['red' if acc < 30 else 'green' for acc in accuracies])\n",
    "\n",
    "  for i in range(len(combined)):\n",
    "        plt.text(i, accuracies[i], accuracies[i], ha = 'center')\n",
    "\n",
    "  plt.title(f'Top 5 and Bottom 5 Class Accuracies for {model_name}')\n",
    "  plt.xlabel('Class')\n",
    "  plt.ylabel('Accuracy (%)')\n",
    "  plt.xticks(rotation=45)\n",
    "  plt.ylim(0, 100)\n",
    "  plt.grid(axis='y')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "id": "hOGjgquWxtj1",
    "outputId": "ca3a45dc-98f8-437d-90c8-5f65feda7a02"
   },
   "outputs": [],
   "source": [
    "plot_accuracy_per_class(\"MLP\", test_mlp_per_class)\n",
    "plot_accuracy_per_class(\"CNN\", test_accuracy_cnn_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5169, 'precision': 0.5154182200485066, 'recall': 0.5169, 'f1': 0.513460343218621}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b864ce0046243dabac8fc9e1e24dcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'apple': 77.0,\n",
       " 'aquarium_fish': 70.0,\n",
       " 'baby': 39.0,\n",
       " 'bear': 30.0,\n",
       " 'beaver': 38.0,\n",
       " 'bed': 51.0,\n",
       " 'bee': 63.0,\n",
       " 'beetle': 56.0,\n",
       " 'bicycle': 60.0,\n",
       " 'bottle': 57.0,\n",
       " 'bowl': 32.0,\n",
       " 'boy': 36.0,\n",
       " 'bridge': 53.0,\n",
       " 'bus': 39.0,\n",
       " 'butterfly': 51.0,\n",
       " 'camel': 48.0,\n",
       " 'can': 56.0,\n",
       " 'castle': 66.0,\n",
       " 'caterpillar': 51.0,\n",
       " 'cattle': 37.0,\n",
       " 'chair': 74.0,\n",
       " 'chimpanzee': 66.0,\n",
       " 'clock': 54.0,\n",
       " 'cloud': 71.0,\n",
       " 'cockroach': 73.0,\n",
       " 'couch': 30.0,\n",
       " 'crab': 53.0,\n",
       " 'crocodile': 37.0,\n",
       " 'cup': 70.0,\n",
       " 'dinosaur': 52.0,\n",
       " 'dolphin': 53.0,\n",
       " 'elephant': 47.0,\n",
       " 'flatfish': 50.0,\n",
       " 'forest': 56.0,\n",
       " 'fox': 56.0,\n",
       " 'girl': 24.0,\n",
       " 'hamster': 53.0,\n",
       " 'house': 45.0,\n",
       " 'kangaroo': 29.0,\n",
       " 'keyboard': 65.0,\n",
       " 'lamp': 46.0,\n",
       " 'lawn_mower': 68.0,\n",
       " 'leopard': 40.0,\n",
       " 'lion': 59.0,\n",
       " 'lizard': 29.0,\n",
       " 'lobster': 31.0,\n",
       " 'man': 26.0,\n",
       " 'maple_tree': 61.0,\n",
       " 'motorcycle': 80.0,\n",
       " 'mountain': 66.0,\n",
       " 'mouse': 24.0,\n",
       " 'mushroom': 51.0,\n",
       " 'oak_tree': 68.0,\n",
       " 'orange': 76.0,\n",
       " 'orchid': 66.0,\n",
       " 'otter': 20.0,\n",
       " 'palm_tree': 72.0,\n",
       " 'pear': 51.0,\n",
       " 'pickup_truck': 62.0,\n",
       " 'pine_tree': 51.0,\n",
       " 'plain': 78.0,\n",
       " 'plate': 48.0,\n",
       " 'poppy': 59.0,\n",
       " 'porcupine': 48.0,\n",
       " 'possum': 22.0,\n",
       " 'rabbit': 33.0,\n",
       " 'raccoon': 50.0,\n",
       " 'ray': 44.0,\n",
       " 'road': 80.0,\n",
       " 'rocket': 76.0,\n",
       " 'rose': 59.0,\n",
       " 'sea': 76.0,\n",
       " 'seal': 25.0,\n",
       " 'shark': 42.0,\n",
       " 'shrew': 37.0,\n",
       " 'skunk': 76.0,\n",
       " 'skyscraper': 72.0,\n",
       " 'snail': 37.0,\n",
       " 'snake': 29.0,\n",
       " 'spider': 59.0,\n",
       " 'squirrel': 28.0,\n",
       " 'streetcar': 62.0,\n",
       " 'sunflower': 74.0,\n",
       " 'sweet_pepper': 43.0,\n",
       " 'table': 29.0,\n",
       " 'tank': 64.0,\n",
       " 'telephone': 49.0,\n",
       " 'television': 57.0,\n",
       " 'tiger': 56.0,\n",
       " 'tractor': 55.0,\n",
       " 'train': 52.0,\n",
       " 'trout': 66.0,\n",
       " 'tulip': 37.0,\n",
       " 'turtle': 25.0,\n",
       " 'wardrobe': 80.0,\n",
       " 'whale': 52.0,\n",
       " 'willow_tree': 41.0,\n",
       " 'wolf': 58.0,\n",
       " 'woman': 26.0,\n",
       " 'worm': 50.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvNetFinal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetFinal, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=96, out_channels=128, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3)\n",
    "        self.fcon1 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fcon2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fcon3 = nn.Linear(in_features=128, out_features=100)\n",
    "        self.drop = nn.Dropout(p=0.55)\n",
    "        self.maxp1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.bn64 = nn.BatchNorm2d(64)\n",
    "        self.bn96 = nn.BatchNorm2d(96)\n",
    "        self.bn128 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.bn64(self.conv1(x)))\n",
    "        x = self.maxp1(x)\n",
    "\n",
    "        x = F.relu(self.bn96(self.conv2(x)))\n",
    "        x = self.maxp1(x)\n",
    "\n",
    "        x = F.relu(self.bn128(self.conv3(x)))\n",
    "\n",
    "        x = F.relu(self.bn128(self.conv4(x)))\n",
    "\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fcon1(x))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = F.relu(self.fcon2(x))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.fcon3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def create_transforms(test=False):\n",
    "    if test:\n",
    "        tr_compose = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "        ])\n",
    "    else :\n",
    "        tr_compose = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5), \n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.2, saturation=0.2),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "        ])\n",
    "    return tr_compose\n",
    "\n",
    "\n",
    "conv3 = ConvNetFinal()\n",
    "checkpoint = torch.load('./models/CONVNET2_MIDLRDECAY_B64.pth')\n",
    "''' \n",
    "optimizer:  torch.optim.SGD(conv3.parameters(), lr=0.003, momentum=0.9, weight_decay=0.001)\n",
    "LR decay:   torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "epochs:     50\n",
    "batch size: 64\n",
    "'''\n",
    "conv3.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "conv3.to(device)\n",
    "\n",
    "trainset = CIFAR100_loader(root='./data', train=True, download=False, transform=create_transforms(test=False))\n",
    "testset = CIFAR100_loader(root='./data', train=False, download=False, transform=create_transforms(test=True))\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(validate(conv3, testloader))\n",
    "validate_per_class(conv3, testloader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhM0lEQVR4nO3dd3hTZf8G8DtpmjRd6V50UKC0FCijMiogQisVEJkyfg6GW1RAfFXkVcBXBVEZKsMJbpaCorKnsimUTSmzQDd07ybP74+SSGgLaUl6mvb+XFcuyMnJyTen6cnd53nOc2RCCAEiIiIiKySXugAiIiKi2mKQISIiIqvFIENERERWi0GGiIiIrBaDDBEREVktBhkiIiKyWgwyREREZLUYZIiIiMhqMcgQERGR1WKQIWoAli5dCplMhosXL9b4udu3b4dMJsP27dvNXpe1mz59OmQyGTIzMy32GuXl5XjttdcQEBAAuVyOQYMGWey16gP9Pq2NMWPGoGnTpuYtiIzU1fFAJpNh+vTpZtlWow4yMpnMpJs5fqCFhYWYPn26yds6efIkpk+fXqsvpppYuHAhli5datHXoArvv/8+1qxZI3UZVM988803+PDDDzFs2DB8++23mDRpktQlNRirV69G37594eHhAaVSCT8/PwwfPhxbt26VujQAwP333w+ZTIYBAwZUeuzixYuQyWT46KOParzd233f6INkVbfFixfX5m1ITiF1AVL6/vvvje5/99132LRpU6XlrVq1uuvXKiwsxIwZMwBUfHjv5OTJk5gxYwbuv/9+i/4FsnDhQnh4eGDMmDEWew2q8P7772PYsGEW+Yv78ccfx8iRI6FSqWr83Pvuuw9FRUVQKpVmr4vubOvWrWjSpAnmzp0rdSkNhhAC48aNw9KlS9GhQwe88sor8PHxQUpKClavXo3o6Gjs2rUL9957r9SlAgD++OMPxMXFITIy0izbM+X7ZtGiRXB0dDRa1qVLFzRv3tzqjgeNOsg89thjRvf37t2LTZs2VVpOZIry8nLodDqzHAAKCgrg4OBg8vo2NjawsbGp1WvJ5XLY2dnV6rl099LT0+Hi4mK27el0OpSWljbqn+nHH3+MpUuXYuLEiZgzZ45RV9bUqVPx/fffQ6GoH19/gYGByMvLw4wZM/D777/X2esOGzYMHh4eVT5mbZ+dRt21ZAqdTod58+ahdevWsLOzg7e3N5599llkZWUZrXfw4EHExsbCw8MDarUawcHBGDduHICKJkJPT08AwIwZMwzNeNX1Dy5duhSPPPIIAKBXr15VdnGtW7cOPXr0gIODA5ycnNC/f3+cOHHCaDupqakYO3Ys/P39oVKp4Ovri4EDBxq6q5o2bYoTJ05gx44dhtcwpbVIb8mSJejduze8vLygUqkQHh6ORYsWVbnuunXr0LNnTzg5OcHZ2RmdOnXCTz/9ZLTOvn370K9fP7i6usLBwQERERGYP3/+Hes4f/48HnnkEbi5ucHe3h5du3bFn3/+aXg8LS0NCoXC8BfKzRISEiCTyfDZZ58ZlmVnZ2PixIkICAiASqVCixYt8MEHH0Cn0xnWubnZd968eWjevDlUKhVOnjxZZY0ymQwFBQX49ttvDfta3wqmb+o9efIk/u///g+urq7o3r07AODo0aMYM2YMmjVrBjs7O/j4+GDcuHG4du2a0farGiPTtGlTPPTQQ/jnn3/QuXNn2NnZoVmzZvjuu++MnltVn/j999+PNm3a4OTJk+jVqxfs7e3RpEkTzJ49u9J7u3TpEh5++GE4ODjAy8sLkyZNwoYNG0zulr169SrGjRsHb29vqFQqtG7dGt98802VNS5fvhxvvvkmfHx84ODggIcffhiXL1+utM2VK1ciMjISarUaHh4eeOyxx3D16tVK650+fRrDhw+Hp6cn1Go1QkNDMXXq1ErrZWdnY8yYMXBxcYFGo8HYsWNRWFhotM6mTZvQvXt3uLi4wNHREaGhoXjzzTerfd/6z9C2bdtw4sSJSr/nBQUFmDx5suFzGBoaio8++ghCCKPtyGQyvPjii/jxxx/RunVrqFQqrF+/vtrX1X8utm/fjnvuuQdqtRpt27Y1vO6vv/6Ktm3bws7ODpGRkTh8+HClbWzdutVw/HFxccHAgQNx6tSpSuv9888/6NSpE+zs7NC8eXN8/vnn1db1ww8/GH5mbm5uGDlyZJU/2zspKirCzJkzERYWho8++qjK8TiPP/44OnfubLh/p2MI8O9ncMWKFXjvvffg7+8POzs7REdH4+zZs4b1XnzxRTg6Olb6fADAqFGj4OPjA61Wa1jm5OSESZMmYe3atTh06NAd39+djk81/b651d0cD0pLS/H2228jMjISGo0GDg4O6NGjB7Zt22bSa9eaIIPx48eLW3fJU089JRQKhXj66afF4sWLxeuvvy4cHBxEp06dRGlpqRBCiLS0NOHq6ipatmwpPvzwQ/Hll1+KqVOnilatWgkhhMjPzxeLFi0SAMTgwYPF999/L77//ntx5MiRKus4d+6cePnllwUA8eabbxrWT01NFUII8d133wmZTCYefPBB8emnn4oPPvhANG3aVLi4uIgLFy4YtnPvvfcKjUYj/vvf/4qvvvpKvP/++6JXr15ix44dQgghVq9eLfz9/UVYWJjhNTZu3Gjy/urUqZMYM2aMmDt3rvj0009Fnz59BADx2WefGa23ZMkSIZPJRJs2bcR7770nFixYIJ566inx+OOPG9bZuHGjUCqVIigoSEybNk0sWrRIvPzyyyImJua2NaSmpgpvb2/h5OQkpk6dKubMmSPatWsn5HK5+PXXXw3r9e7dW4SHh1d6/owZM4SNjY1h3xYUFIiIiAjh7u4u3nzzTbF48WLxxBNPCJlMJiZMmGB43oULFwQAER4eLpo1ayZmzZol5s6dKy5dulRlnd9//71QqVSiR48ehn29e/duIYQQ06ZNM2xr4MCBYuHChWLBggVCCCE++ugj0aNHD/HOO++IL774QkyYMEGo1WrRuXNnodPpjPYxAKOff1BQkAgNDRXe3t7izTffFJ999pno2LGjkMlk4vjx44b1tm3bJgCIbdu2GZb17NlT+Pn5iYCAADFhwgSxcOFC0bt3bwFA/PXXX4b18vPzRbNmzYRarRZvvPGGmDdvnujcubNo165dpW1W9/Pz9/cXAQEB4p133hGLFi0SDz/8sAAg5s6dW6nGtm3bioiICDFnzhzxxhtvCDs7O9GyZUtRWFhYaV906tRJzJ07V7zxxhtCrVaLpk2biqysLMN6R44cEc7OzsLd3V1MmTJFfP755+K1114Tbdu2Nayj/9l06NBBDBkyRCxcuFA89dRTAoB47bXXDOsdP35cKJVKcc8994j58+eLxYsXi1dffVXcd9991b73/Px88f3334uwsDDh7+9v9Huu0+lE7969hUwmE0899ZT47LPPxIABAwQAMXHiRKPtABCtWrUSnp6eYsaMGWLBggXi8OHD1b6u/nPh6+srpk+fLubOnSuaNGkiHB0dxQ8//CACAwPFrFmzxKxZs4RGoxEtWrQQWq3W8PxNmzYJhUIhWrZsKWbPni1mzJghPDw8hKurq9Hn7+jRo0KtVovAwEAxc+ZM8b///U94e3uLiIiISsfZd999V8hkMjFixAixcOFCwzZv/ZmNHj1aBAUFVfvehKg4lgAQ77zzzm3X0zP1GKL/DHbo0EFERkaKuXPniunTpwt7e3vRuXNnw3o7d+4UAMSKFSuMXqegoEA4ODiI8ePHG5b17NlTtG7dWuTk5AhXV1cxYMAAw2P6Y8yHH35otI07HZ/u9H2j/0wnJCSIjIwMw+369etG77M2x4OMjAzh6+srXnnlFbFo0SIxe/ZsERoaKmxtbSt9JgGIadOmmfQzuhMGmZvcGmT+/vtvAUD8+OOPRuutX7/eaPnq1asFAHHgwIFqt52RkVGjH9zKlSur/CLIy8sTLi4u4umnnzZanpqaKjQajWF5VlZWpV+CqrRu3Vr07NnTpJpudfOXh15sbKxo1qyZ4X52drZwcnISXbp0EUVFRUbr6r+Iy8vLRXBwsAgKCjI6aN28TnUmTpwoAIi///7bsCwvL08EBweLpk2bGg7An3/+uQAgjh07ZvT88PBw0bt3b8P9//3vf8LBwUGcOXPGaL033nhD2NjYiKSkJCHEvwcZZ2dnkZ6eftsa9RwcHMTo0aMrLdcfWEaNGlXpsar28c8//ywAiJ07dxqWVRdkbl0vPT1dqFQqMXnyZMOy6g5cAMR3331nWFZSUiJ8fHzE0KFDDcs+/vhjAUCsWbPGsKyoqEiEhYWZFGSefPJJ4evrKzIzM42Wjxw5Umg0GsP719fYpEkTkZuba1hvxYoVAoCYP3++EEKI0tJS4eXlJdq0aWP0efvjjz8EAPH2228blt13333CycmpUvi8+TOn/9mMGzfOaJ3BgwcLd3d3w/25c+cKACIjI+O277cq+i+zm61Zs0YAEO+++67R8mHDhgmZTCbOnj1rWAZAyOVyceLECZNeT/+50AdpIYTYsGGDACDUarXR/tD/3tz8c2zfvr3w8vIS165dMyw7cuSIkMvl4oknnjAsGzRokLCzszPa3smTJ4WNjY3RcfbixYvCxsZGvPfee0Z1Hjt2TCgUCqPlpgSZ+fPnCwBi9erVd9wXQph+DNF/Blu1aiVKSkoqvZ7+2KLT6USTJk2Mfk+E+PezevPv480/+xkzZggAIi4uTghRdZAx9fh0u+8b/Wf61pt+v97N8aC8vNxo3whR8V3k7e1d6XfInEGGXUu3sXLlSmg0GjzwwAPIzMw03CIjI+Ho6GhoLtP3b//xxx8oKyuzaE2bNm1CdnY2Ro0aZVSTjY0NunTpYqhJrVZDqVRi+/btlbrBzEWtVhv+n5OTg8zMTPTs2RPnz59HTk6Ood68vDy88cYblfpd9U2+hw8fxoULFzBx4sRKYwXudJrmX3/9hc6dOxu6YgDA0dERzzzzDC5evGjo6hkyZAgUCgWWL19uWO/48eM4efIkRowYYVi2cuVK9OjRA66urkb7NyYmBlqtFjt37jR6/aFDhxqace/Wc889V2nZzfu4uLgYmZmZ6Nq1KwCY1AwdHh6OHj16GO57enoiNDQU58+fv+NzHR0djcaLKZVKdO7c2ei569evR5MmTfDwww8bltnZ2eHpp5++4/aFEPjll18wYMAACCGM9ndsbCxycnIqvccnnngCTk5OhvvDhg2Dr68v/vrrLwAVXbzp6el44YUXjD5v/fv3R1hYmKG7ICMjAzt37sS4ceMQGBho9BpVfeZu/dn06NED165dQ25uLoB/jwG//fabURdkbf3111+wsbHByy+/bLR88uTJEEJg3bp1Rst79uyJ8PBwk7cfHh6OqKgow/0uXboAAHr37m20P/TL9T/zlJQUxMfHY8yYMXBzczOsFxERgQceeMDwc9BqtdiwYQMGDRpktL1WrVohNjbWqJZff/0VOp0Ow4cPN/oM+Pj4ICQkpMbdEvqfyc2fk9sx9RiiN3bsWKNxcPrfL/0+kslkeOSRR/DXX38hPz/fsN7y5cvRpEkTo9e52YQJE+Dq6lplF7heTY9Pt/PLL79g06ZNhtuPP/542/VNOR7Y2NgY9o1Op8P169dRXl6Oe+65x6TjVW0xyNxGYmIicnJy4OXlBU9PT6Nbfn4+0tPTAVQcRIYOHYoZM2bAw8MDAwcOxJIlS1BSUmKRmoCKA86tNW3cuNFQk0qlwgcffIB169bB29sb9913H2bPno3U1FSz1bJr1y7ExMQY+sk9PT0NYwL0QebcuXMAgDZt2lS7HVPWqc6lS5cQGhpaabn+TLNLly4BADw8PBAdHY0VK1YY1lm+fDkUCgWGDBliWJaYmIj169dX2rcxMTEAYNi/esHBwTWuuTpVbev69euYMGECvL29oVar4enpaVhPv49v59YvaQBwdXU1Kdz6+/tX+lK/9bmXLl1C8+bNK63XokWLO24/IyMD2dnZ+OKLLyrt77FjxwKovL9DQkKM7stkMrRo0cIwNkj/867qMxEWFmZ4XH/wNfUzd+t+dHV1BQDDvhgxYgS6deuGp556Ct7e3hg5ciRWrFhR61Bz6dIl+Pn5VfoyvvVzrVfTz+Gt70ej0QAAAgICqlyuf5+327+tWrVCZmYmCgoKkJGRgaKioko/r6qem5iYCCEEQkJCKn0OTp06VekzcCfOzs4AgLy8PJPWN/UYonenzwJQ8XkoKioyDN7Nz8/HX3/9hUceeaTaP840Gg0mTpyI33//vcpxSUDNj0+3c9999yEmJsZw69at223XN+V4AADffvstIiIiYGdnB3d3d3h6euLPP/806XhVW/Vj2HY9pdPp4OXlVW1S1f8lLpPJsGrVKuzduxdr167Fhg0bMG7cOHz88cfYu3dvpVPc7rYmoOLUcR8fn0qP3zwSf+LEiRgwYADWrFmDDRs24K233sLMmTOxdetWdOjQ4a7qOHfuHKKjoxEWFoY5c+YgICAASqUSf/31F+bOnWuWv0rNbeTIkRg7dizi4+PRvn17rFixAtHR0UYj93U6HR544AG89tprVW6jZcuWRvdvbjG5W1Vta/jw4di9ezf+85//oH379nB0dIROp8ODDz5o0j6u7kwmccuAUXM/1xT6+h977DGMHj26ynUiIiLM8lp36077Qq1WY+fOndi2bRv+/PNPrF+/HsuXL0fv3r2xcePGWp9RZqqafg6rq8fSP/Oq6HQ6yGQyrFu3rsrXr+nxMywsDABw7Ngxi0x1YMo+6tq1K5o2bYoVK1bg//7v/7B27VoUFRUZtf5WZcKECZg7dy5mzJiBefPmVXq8pscnczLlff/www8YM2YMBg0ahP/85z/w8vKCjY0NZs6cafiD1RIYZG6jefPm2Lx5M7p162bSgaJr167o2rUr3nvvPfz000949NFHsWzZMjz11FM1nsmyuvWbN28OAPDy8jKk8Du9h8mTJ2Py5MlITExE+/bt8fHHH+OHH3647evcydq1a1FSUoLff//d6C+UW5uB9fUeP3682r/Sb17HlPd0s6CgICQkJFRafvr0acPjeoMGDcKzzz5r6F46c+YMpkyZUqmW/Pz8Gtdhipru66ysLGzZsgUzZszA22+/bViub5WrD4KCgnDy5EkIIYze381ncVTH09MTTk5O0Gq1Ju/vW9+7EAJnz541BB79zzshIQG9e/c2WjchIcHweLNmzQBUfObMRS6XIzo6GtHR0ZgzZw7ef/99TJ06Fdu2bavV53rz5s3Iy8szapWp6nNdl27ev7c6ffo0PDw84ODgADs7O6jV6io/q7c+t3nz5hBCIDg42CxfxN27d4erqyt+/vlnvPnmm3cMkTU5htTE8OHDMX/+fOTm5mL58uVo2rSpoVu4OvpWmenTp1cZ7k09PtX2uH63Vq1ahWbNmuHXX381qmHatGkWfV12Ld3G8OHDodVq8b///a/SY+Xl5cjOzgZQ8YVz618s7du3BwBD95K9vT0AGJ5zJ/o5RG5dPzY2Fs7Oznj//ferHI+TkZEBoGJCpOLiYqPHmjdvDicnJ6MuLwcHB5Nrupn+4HDz+87JycGSJUuM1uvTpw+cnJwwc+bMSvXon9uxY0cEBwdj3rx5lWq501+C/fr1w/79+7Fnzx7DsoKCAnzxxRdo2rSp0bgBFxcXxMbGYsWKFVi2bBmUSmWlv9iGDx+OPXv2YMOGDZVeKzs7G+Xl5bet53Zquq+r2scAqvxLTSqxsbG4evWq0fwXxcXF+PLLL+/4XBsbGwwdOhS//PJLlYFC/1m+2XfffWfUZbBq1SqkpKSgb9++AIB77rkHXl5eWLx4sdHnfN26dTh16hT69+8PoCJE3Xffffjmm2+QlJRk9Bq1aX24fv16pWW3HgNqol+/ftBqtUbTAgDA3LlzIZPJDO+3rvn6+qJ9+/b49ttvjT7Lx48fx8aNG9GvXz8AFT/b2NhYrFmzxmj/njp1qtLv1pAhQ2BjY4MZM2ZU2vdCiEpTDdyJvb09Xn/9dZw6dQqvv/56lT/PH374Afv37wdQs2NITYwYMQIlJSX49ttvsX79egwfPtyk5+nHCr7zzjuVHjP1+FTT7xtzqeqYtW/fPqN9awlskbmNnj174tlnn8XMmTMRHx+PPn36wNbWFomJiVi5ciXmz59vmFZ84cKFGDx4MJo3b468vDx8+eWXcHZ2Nvxiq9VqhIeHY/ny5WjZsiXc3NzQpk2bavvo27dvDxsbG3zwwQfIycmBSqUyzNmyaNEiPP744+jYsSNGjhwJT09PJCUl4c8//0S3bt3w2Wef4cyZM4iOjsbw4cMRHh4OhUKB1atXIy0tDSNHjjS8TmRkJBYtWoR3330XLVq0gJeXV6W/ZKvSp08fKJVKDBgwAM8++yzy8/Px5ZdfwsvLCykpKYb1nJ2dMXfuXDz11FPo1KmTYZ6UI0eOoLCwEN9++y3kcjkWLVqEAQMGoH379hg7dix8fX1x+vRpnDhxospfWr033ngDP//8M/r27YuXX34Zbm5u+Pbbb3HhwgX88ssvkMuNs/qIESPw2GOPYeHChYiNja00uPg///kPfv/9dzz00EMYM2YMIiMjUVBQgGPHjmHVqlW4ePFitZNI3UlkZCQ2b96MOXPmwM/PD8HBwYbBlFVxdnY2jG0qKytDkyZNsHHjRly4cKFWr28Jzz77LD777DOMGjUKEyZMgK+vL3788UfDQNs7/WU4a9YsbNu2DV26dMHTTz+N8PBwXL9+HYcOHcLmzZsrBQQ3Nzd0794dY8eORVpaGubNm4cWLVoYBhfb2trigw8+wNixY9GzZ0+MGjUKaWlpmD9/Ppo2bWo0/f8nn3yC7t27o2PHjnjmmWcQHByMixcv4s8//0R8fHyN9sM777yDnTt3on///ggKCkJ6ejoWLlwIf3//agd33s6AAQPQq1cvTJ06FRcvXkS7du2wceNG/Pbbb5g4caKhFVMKH374Ifr27YuoqCg8+eSTKCoqwqeffgqNRmM0V8mMGTOwfv169OjRAy+88ALKy8vx6aefonXr1jh69KhhvebNm+Pdd9/FlClTcPHiRQwaNAhOTk64cOECVq9ejWeeeQavvvpqjWr8z3/+gxMnTuDjjz/Gtm3bMGzYMPj4+CA1NRVr1qzB/v37sXv3bgA1P4aYqmPHjmjRogWmTp2KkpKSO3Yr6Wk0GkyYMKHKQb+mHp9q+n1jLg899BB+/fVXDB48GP3798eFCxewePFihIeHGw18NjuznPvUQFQ1j4wQQnzxxRciMjJSqNVq4eTkJNq2bStee+01kZycLIQQ4tChQ2LUqFEiMDBQqFQq4eXlJR566CFx8OBBo+3s3r1bREZGCqVSadKpZ19++aVo1qyZ4XTFm0+H27Ztm4iNjRUajUbY2dmJ5s2bizFjxhheMzMzU4wfP16EhYUJBwcHodFoRJcuXSrNbZCamir69+8vnJycBIAanYr9+++/i4iICGFnZyeaNm0qPvjgA/HNN99UOg1Yv+69994r1Gq1cHZ2Fp07dxY///yz0Tr//POPeOCBB4STk5NwcHAQERER4tNPP71jHefOnRPDhg0TLi4uws7OTnTu3Fn88ccfVa6bm5sr1Gq1ACB++OGHKtfJy8sTU6ZMES1atBBKpVJ4eHiIe++9V3z00UeGuYOqOjXyTk6fPi3uu+8+w+vrT8XWnw5Z1am7V65cEYMHDxYuLi5Co9GIRx55RCQnJ1f6/FR3+nX//v0rbbNnz55GP+fqTre89ZRgIao+/fX8+fOif//+Qq1WC09PTzF58mTxyy+/CABi7969d9wvaWlpYvz48SIgIEDY2toKHx8fER0dLb744otKNf78889iypQpwsvLS6jVatG/f/8q5+5Zvny56NChg1CpVMLNzU08+uij4sqVK5XWO378uGH/2tnZidDQUPHWW28ZHq/uZ3Pr/t6yZYsYOHCg8PPzE0qlUvj5+YlRo0ZVOk22KtXt67y8PDFp0iTh5+cnbG1tRUhIiPjwww8rTUkAwGhukjup7nNR1Xaq+5xv3rxZdOvWzfD7PGDAAHHy5MlK29yxY4fhmNesWTOxePFiwz691S+//CK6d+8uHBwchIODgwgLCxPjx48XCQkJhnVMOf36ZqtWrRJ9+vQRbm5uQqFQCF9fXzFixAixfft2o/VMOYboP4MrV66sch8tWbKk0utPnTpVABAtWrSosr7qfvZZWVlCo9FUue9NOT4JUf33ze2ONze/z9ocD3Q6nXj//fdFUFCQUKlUokOHDuKPP/6o8udmynegqWQ3NkhEZDbz5s3DpEmTcOXKFTRp0uSut7d9+3b06tULK1euxLBhw8xQIRE1FBwjQ0R3paioyOh+cXExPv/8c4SEhJglxBAR3Q7HyFAld5prRq1WG+aXIBoyZAgCAwPRvn175OTk4IcffsDp06fvOMEWEZE5MMhQJb6+vrd9fPTo0Vi6dGndFEP1XmxsLL766iv8+OOP0Gq1CA8Px7Jly0we3EhEdDc4RoYq2bx5820f9/Pzq/UpiURERObEIENERERWi4N9iYiIyGo1+DEyOp0OycnJcHJykmzaZiIiIqoZIQTy8vLg5+d324kJG3yQSU5OrnRFVyIiIrIOly9fhr+/f7WPN/ggo7/g2uXLlw2XdyciIqL6LTc3FwEBAUYXTq1Kgw8y+u4kZ2dnBhkiIiIrc6dhIRzsS0RERFaLQYaIiIisFoMMERERWS0GGSIiIrJaDDJERERktRhkiIiIyGoxyBAREZHVYpAhIiIiq8UgQ0RERFaLQYaIiIisFoMMERERWS0GGSIiIrJaDDJERNToFJdppS6BzIRBhoiIGpWv/j6P1tM24NWVRxhoGgAGGSIiajQ2nkjFe3+dglYnsCruCkZ9uRfpecVSl0V3gUGGiIgahVMpuZi4PB5CAA+Ee0OjtsXhpGwM/GwXjl/Nkbo8qiUGGSIiavCu5ZfgqW8PorBUi24t3LHw0Y5YM74bmns6ICWnGMMW78afR1OkLpNqgUGGiEhiQghcyCzgeA0LKS3X4bkf4nA1uwhN3e2x4P86wtZGjmAPB6we3w09W3qiuEyH8T8dwpxNZ6DTCalLphpQSF0AEVFjN3PdaXyx8zxsbWRo7afBPUGuiLxx83K2k7q8eiGroBSbT6VBAHi4nR/sbG1Mep4QAv9dcwwHLmbByU6Br0Z3gou90vC4s50tvhnTCbPWncKXf1/AJ1sSkZiWh4+Ht4O9su6+Is9l5CM+KRsyGWAjl0Ehl0NhI4NCLjPct5HLoLKVo7WfM1QK095/XSgt10Ehl0Eul0ny+jIhRIOOnrm5udBoNMjJyYGzs7PU5RARGfkl7gomrzxS7eP+rmpDqOkQ4ApbhQxZBWXIKSpFVmEZsgpLkXPj36zCMuQUlsHfVY0hHf0R1dwdNhb8crmWX4J9F66jjZ8Gge72Zt9+em4xNpxMw/rjKdh7/jq0N1pKgtztMf3h1ugV6nXHbXz9zwX874+TkMuAb8Z0wv23ec6Kg5cxdfUxlGkFWvk648snIuHvavy+isu0SM8tQWpuMVJzi5GeWwyN2hZdm7kjwK1m+yA5uwhrjyTj9yPJOJGca/LznO0U6NfWF4M6NEHnpm6SBIiCknLsOJOBDSdSsfV0Or4Z0wmdmrqZ9TVM/f5mkCEikkj85WwM/3wPSst1eKl3CzwSGYC4pOuIu5SFuEvZSEjNxd30cvhq7DCkYxMM7eiPZp6OZqv7SlYhvtx5HssPXkZxmQ4A0KmpK4Z09Ee/tr7QqG3vatvrj6di/fFUxCVl4eZvqFa+zriWX4L0vBIAFQN2334ovNoAsT0hHeOWHoBOAG89FI4nuwff8fUPXryO536IQ2Z+KdwdlIhp5Y3U3GKk3bhlFZZV+9wmLmp0beaOrs3cENXcvVIIAirC31/HU7E2Phn7L143LLeRyxAZ5AqVQg6tTqBcJ276V4dybcX96wWluFZQanien8YOA9r7YXCHJgjzsex33PUbrWIbT6RiZ2ImSst1hsfG92qO/8SGmfX1GGRuYJAhovooPbcYAz77B2m5JYhp5Y0vHo+s9Jd1XnEZjlzOqQg2SVk4eiUbcpkMLva2cLVXwtXeFhp1xb+uDkpo1LZwVtti/4VrWHskBTlF/37pdgh0wbBIfzwU4VfroJGQmofPd5zDb0eSDa0jAW5qXMkqMgQOpUKOmFZeGNLBHz1DPWFrU/1QzJJyLS5mFiIxPQ9n0vKxPSEdR68Ynz3UPsAFfdv44ME2Pghyd0B+STk+2ZKIb/65gHKdgEohx/heLfDMfc2MupvOpudj8IJdyCspx4h7AjBraFvIZKa1XFzJKsTT38XhVErVrSQqhRw+Gjt4O1fcUrKLEH85G+W3pE5/V32wcYcMwNqjyfg7MdOw7wCgc1M3PNzeD/3a+sLNQYk70eoE9l24ht8OJ+OvYynIKyk3PBbm44SB7ZtgYHs/+LmoodUJlJRrUVKmQ/GNf0vKdSgu06KkXAeZrOK9qBQ2sLOt+FelkMPO1gZKRUVX1tXsImw4nooNJ1Jx4OJ1o2Ad6GaP2NbeiG3tgw6BrmZv/bOKIDN9+nTMmDHDaFloaChOnz4NACguLsbkyZOxbNkylJSUIDY2FgsXLoS3t7fJr8EgQ0T1TUm5FiO/2IvDSdkI8XLEry/cCye72rdiVPcaW06lY1XcFew4k2H48lQq5Hgg3Bv92viiuZcDAt3s7zgWJO5SFhZtP4vNp9INy7q38MDz9zfHvc3dkZpbjN/ik7H60FUkpOUZ1nFzUGJAREUXiEIux9mMPCSm5eNsesXt0vVCoy91AJDJgE5N3dC3jQ9iW/vAz0VdZU2JaXl4+7cT2HP+GoAb3U0DWqNXmBeyC0sxaMEuXLxWiM5N3fDDU12gVNTs3JaCknL8uO8Sikp18NGo4O1sBx+NHXyc7aBR21YKRYWl5Yi7lIW9569hz7lrOHolp1Kw0WvTxBkPt/PDQxF+1b4/UxSXabHtdDrWxF/FttMZKNX+20KikMuqfX1T2drIUKY13ka4rzNiW/sgto03Qr2dTA6HtWE1QWbVqlXYvHmzYZlCoYCHhwcA4Pnnn8eff/6JpUuXQqPR4MUXX4RcLseuXbtMfg0GGSKqT4QQeG3VUayMuwJnOwV+f7E7mno4WPQ10/OK8dvhZKyKu2IUNPQ8nVQIcrNHoLs9gtwc0NTDHoFu9sgqLMXiHeex/0JFF4hMBjzY2gfP398cEf4uVb63kym5+PXQVfwWn4zM/JI71uakUqC5lyNaeDmiQ6AL+oT7wNNJZdL7EkJg7dEUvPfnSaTlVrxWTCtv5JeUYe/56/B3VeO38d3g7mja9sypoOSmYHP+GorLdOgT7o2H2/uhuRm7+fRyCsuw7ngKVh++in0Xrld6XCGXwc7W5kYLjByqG61XJWVaFJfrUHKjlebW8COXAfc0dUOf8IqWl5qOA7obVhNk1qxZg/j4+EqP5eTkwNPTEz/99BOGDRsGADh9+jRatWqFPXv2oGvXria9BoMMUf2XnleMzLxSqGzlhqZtfZO3rY3sjn/1CSGgE0C5ruIv0vp0Rsetlu66gOlrKwafLh3bGfe19Kyz1xZC4ERyLlbFXcHhpCxcul6I7NuM+dCztZFhSAd/PNOzmclfwuVaHf45m4lfD13FppNpsFfaoMWNwBLi5YgWXk5o4eUIb2fVXf9Vf2t3EwA4KG3wywv3WnzcSH2UVVCK4nIt7BQ2UNnKobSRQ3GbLr6blWsrup8qblrY2yqgsTdva6GpTP3+lvz068TERPj5+cHOzg5RUVGYOXMmAgMDERcXh7KyMsTExBjWDQsLQ2BgYI2CDBHVLyXlWpxMzsWhpGwcTsrC4aRsXM0uqnZ9uQyGPnyFzY2BkFpdpQGRN2vt54yeLT1xf6gXOgS63HacRl3afS4T//vzFADgzX6t6jTEAIBMJkObJhq0aaIxLMspLMOl6wW4dK0QSdcLcelaAS5eK0TStUKUlGsxLNIfT3ZvBh9NzU4DV9jIcX+oF+4P9YIQwqJdEI4qBd7s1wqPRPpj+toTOJyUjfkjOzTKEAMAriaMtamO4kbocaj7RqxakzTIdOnSBUuXLkVoaChSUlIwY8YM9OjRA8ePH0dqaiqUSiVcXFyMnuPt7Y3U1NRqt1lSUoKSkn+bM3NzTT+ljYj+lV1YijKtqJjH4pb5LOQyGH0x6XQCZTodSst1KNMKlGkr/l9649+z6fk4nJSNw5ezcOJqrlFfPlDRZeHuoERpuQ7F5TqjsyF0Aigq06KoBpPFnUjOxYnkXCzcfg5Odgp0b+GBni090TPUE76a2o9JuBuXrxdi/I+HoNUJDOnQxKQzaOqCxt4WEfYuVXYVmYslQ8zNQryd8ONTXaHVCYuedk71i6RBpm/fvob/R0REoEuXLggKCsKKFSugVtfuYDNz5sxKA4iJqGaW7U/CG78eu+06tjYyyGWyKltE7sTNQYkOAS7oEOiCDoGuiPDXGA121ekESvVN3Df67kvKtSgtF4ZJwhRyuVHAsr1xv7CkHLvOZWJ7QgZ2nslAVmEZ1h1PxbrjFX8AhXo74f5QT3Rt5o72AS539derqQpKyvH0dweRVViGdv4avD/E9DNoqOYYYhqXenf6dadOnRATE4MHHngA0dHRyMrKMmqVCQoKwsSJEzFp0qQqn19Vi0xAQADHyBCZSAiBB+buxNn0/Fpvw0Yug62NDEobOWxt5PBzUaPjjdDSIdAFgW72dfJFrtUJHLuag+0J6dhxJgPxl7Nx6xEv2MPBKFSF+jhV2xWl1Qmk5hbjalYRrmYX4mpWEYrLdHCxt4VGfeOUaAdbuNgr4aKuWGYjl+GFHw9h3fFUeDiqsPalbpK1ChFZE6sZI3Oz/Px8nDt3Do8//jgiIyNha2uLLVu2YOjQoQCAhIQEJCUlISoqqtptqFQqqFRW1LlHVM8cvZKDs+n5UCnkOPDfGDgqFTeNRfl3bEq5tuK+Qi6HUiGHrY0MtjeCS335i9hGLkP7ABe0D3DBxJiWyCooxc7EDPydmIlDSVk4n1GAC5kVt18PXwUA2NnKEdGkItioFHJcyS66EVyKkJpTXOPWJ0eVAvkl5bC1keHzxzsyxBCZmaRB5tVXX8WAAQMQFBSE5ORkTJs2DTY2Nhg1ahQ0Gg2efPJJvPLKK3Bzc4OzszNeeuklREVFcaAvkQWtirsCAHiwjQ+cb3T3KA3BpP6eDWQKVwfljQnDmgCoGAcUfzn7xvidbMQnZSG3uBz7L143mnX1Zgq5DL4udmjiokYTF3s4qGyQU1SGrMIyZBeWIvvG5QLyiismKsu/MWHZu4PaIDLIvFO4E5HEQebKlSsYNWoUrl27Bk9PT3Tv3h179+6Fp2fFSP65c+dCLpdj6NChRhPiEZFlFJdp8fuRZADAsEh/iauxPBd7peHMGqBibM75zAIcTspC/OVs6ETF7Kz+ruqK4OKqhpeTnUktTuVanSHgKOQyi88VQ9RY1bsxMubGeWSITPfn0RSM/+kQfDV2+Of13vWmi4iIGh9Tv7/rx+QKRFQv/HKooltpSMcmDDFEZBUYZIgIQMVFDHecyQAADOnY8LuViKhhYJAhIgDAmvir0OoEOga6WORaMERElsAgQ0QQQhjOVhoWGSBxNUREpmOQISIcv5qLM2kVc8f0j/CVuhwiIpMxyBARVsVdBgDEtvaBRi3NlW6JiGqDQYaoAcktLkNmfsmdV7xJSbkWv92YO2ZoI5g7hogaFgYZogYir7gM/T/5G/fN3objV3NMft7WU+nILiyDt7MK3Vt4WLBCIiLzY5AhaiA+3JCAy9eLUFiqxXM/xCGroNSk5+kH+Q7p6M+5Y4jI6jDIEDUABy9ex/d7LwEAPBxVuJJVhAnL46G9wwUOM/JKsP3G3DFDOXcMEVkhBhkiCeWXlGPr6TTM2XQGJ5JN7w66WUm5Fm/8egxCAMPv8cf3T3aGna0cO89kYO6mM7d97m835o5pH+CCFl6cO4aIrI+kF40kamyKy7Q4lJSF3WevYfe5TBy5kmNoNfluz0X8+vy9aFbDyegWbT+Hs+n58HBU4s1+reBir8SsIRGYuDwen207iwh/Dfq09qn0PCEEVh7Uzx3D1hgisk4MMkQWVKbV4djVHOw5dw27zmbi4KUslJbrjNYJdLOHjVyGC5kFGLf0AH59oRvcHJQmbT8xLQ8Ltp0FAEx/uDVc7CueN6hDE8RfzsbS3RcxecUR/PaiY6WAdCI5FwlpeVAq5BgQ4WeGd0tEVPcYZIjMqLhMiyOXs7H/wnXsv3gdcZeyUFiqNVrHy0mFbi08ENXcHVHN3BHgZo+MvBIMXrgLF68V4tnvD+KHp7pApbC57WvpdAKv/3IUZVqBmFZe6N/WeCK7qf1b4URyDg5czMKz38dhzfhucFD9+yuvH+TbJ9wbGnvOHUNE1olBhuguFJSU41BSFvZfuI59F64j/nJ2pRYXF3tbdA12x70t3HFvcw8093SATGZ8dpCnkwpLxnTCkEW7ceBiFl5bdRTzRrSvtN7Nfth3CYeSsuGoUuB/g9pUWtfWRo4F/9cRD336DxLT8/HaL0fx2agOkMlkKC3X4bf4qwDYrURE1o1BhqgWhBCYvSEBX+48j/JbzgzycFShSzM3dAl2Q5dgd4R4OUJuwmnNId5OWPRoJMYs2Y/f4pPR1N0Bkx5oWeW6ydlFmL0+AQDw+oOh8NWoq1zPy9kOix7riBGf78WfR1PQIcAFT/Vohq2n05FVWAYvJxV6hHjW8N0TEdUfDDJEtfDZ1rNYtP0cAKCJixpdgt3Q+cYt2KNyi4upuod44N1BbfDGr8cwf0sigtztMeSW06KFEHhrzXHkl5QjMsgVj3YJuu02I4Pc8NZD4Zj2+wnMXHcarf00hm6lwR2bcO4YIrJqDDJENbRsfxI+vnFa87QB4RjbLdis2x/ZORAXrxVi8Y5zeP2XoxVBqZm74fE/j6Vgy+l02NrIMGtIW5Nae56ICkL85WysPnwVL/50CDlFZQCAYZw7hoisHOeRIaqBTSfT8ObqYwCAF+5vbvYQo/dabCj6tfVBmVbgme/jcD4jHwCQXViK6b+fAACM79UCId5OJm1PJpPh/cFt0crXGdcKSlGuE2gX4GLy84mI6isGGSITxV26jhd/OgSdAB6J9Md/YkMt9lpyuQxzhrdH+wAX5BSVYdzSA7heUIr3/zqFzPxShHg54vn7m9dom2qlDT5/LNJwdWsO8iWihkAmhLj9HOZWLjc3FxqNBjk5OXB2dpa6HLJSiWl5GLZ4D3KKytA7zAtfPB4JhY3l/w7Qn5Z9JasIzT0dcC6jADIZsOq5KEQGudVqm8ev5uDvxEw81SMYtnXwHoiIasPU728exYjuICWnCE98sx85RWVoH+CCz/6vQ52EGODf07Kd7BQ4l1EAAHi8a1CtQwwAtGmiwfP3N2eIIaIGgUcyajSEEDifkY8f913C70eSkVNYdsfn5BSWYfQ3+5GSU4xmng74Zkwn2Cvrdoy8/rRspY0cgW72Fu3SIiKyNjxriRq0nKIy7D6biZ2Jmdh5JgNXs4sMj9nIZbgnyBUxrbwR3cqr0hT+xWVaPPXdAZxJy4e3swrfjets8qUDzK17iAd2vdEb9kobo9l5iYgaO46RoQalXKvDkSs5+DsxAzvPZCD+cjZunq9OaSNHxyAXXMsvRWJ6vtFzgz0cEB3mhehW3ugQ6IKXfz6MjSfT4GSnwMrnohDmw88PEVFdMfX7m0GGGoSE1Dz8vD8Ja+KvIvuWLqPmng7oEeKJni090aWZm6FrKOlaIbacTsOWU+nYd+EayrT//iooFXKUluugVMjx3bjO6HrTPC5ERGR5DDI3MMg0XIWl5fjjaAp+3p+Ew0nZhuUatS26t/BAjxAP9GjpiSYuVU/ff7O84jL8nZiJzafSsO3G9P0yGbDw/zqi7y0XYyQiIssz9fubne1kdU4k5+Dn/Un47XAy8krKAQAKuQwPhHtjZOdAdG/hUeNp953sbNGvrS/6tfWFVicQfzkbKoUcbZpoLPEWiIjITBhkyCpodQIrD17GT/uTcPRKjmF5oJs9RnYOwLBIf3g52ZnltWzkMkQGuZplW0REZFkMMmQVPt2aiHmbEwEAtjYy9Gntg//rHIioZu4mXWuIiIgaJgYZqveEEFh5sOJqzU/3CMZzPZvD3VElcVVERFQfMMhQvXf8ai6uZhdBbWuDVx4IhVppI3VJRERUT3BmX6r31h1PAQD0CvNkiCEiIiMMMlSvCSGw/ngqAODBNjwNmoiIjDHIUL12Ji0f5zMLoFTI0TvMS+pyiIionmGQoXrtr2MV3Ur3hXjAkdcYIiKiWzDIUL2m71bqy24lIiKqAoMM1VvnM/KRkJYHhVyGmFbeUpdDRET1EIMM1VvrbrTG3NvCAxp7W4mrISKi+ohBhuqtf7uVfCSuhIiI6isGGaqXLl8vxLGrOZDLgD7h7FYiIqKqMchQvaRvjekc7MbLERARUbUYZKhe0s/m268tz1YiIqLqMchQvZOaU4xDSdkAgNjWHB9DRETVY5ChemfDiYpupcggV3g720lcDRER1WcMMlTv6Gfz5dlKRER0JwwyVK9k5pfgwMXrANitREREd8YgQ/XKxhNp0Akgwl+DADd7qcshIqJ6jkGG6hX92UoPsluJiIhMwCBD9UZ2YSn2nLsGgBeJJCIi0zDIUL2x6WQaynUCYT5OCPZwkLocIiKyAgwyVG/oZ/NltxIREZmKQYbqhbziMvydmAmA3UpERGQ6BhmqF7aeTkepVodmng5o6e0odTlERGQlGGSoXtB3K/Vt4wOZTCZxNUREZC0YZEhyhaXl2JaQDoDdSkREVDMMMiS5HQkZKC7Twd9VjdZ+zlKXQ0REVoRBhiSl1Qn8fOAyAHYrERFRzTHIkGR0OoE3fjmKnWcyYCOXYUhHf6lLIiIiK8MgQ5LQ6QSmrjmOlXFXIJcB80e2RytfdisREVHNMMhQnRNCYNrvJ/Dz/iTIZcDcEe3xUISf1GUREZEVYpChOiWEwDt/nMT3ey9BJgM+HNYOA9s3kbosIiKyUgwyVGeEEJi57jSW7LoIAJg1pC2GRnJcDBER1R6DDNUJIQQ+3JCAL3aeBwC8O6gNRnQKlLgqIiKydvUmyMyaNQsymQwTJ040LCsuLsb48ePh7u4OR0dHDB06FGlpadIVSbU2b3MiFm4/BwCY8XBrPNY1SOKKiIioIagXQebAgQP4/PPPERERYbR80qRJWLt2LVauXIkdO3YgOTkZQ4YMkahKqq3PtiZi/pZEAMB/+7fC6HubSlsQERE1GJIHmfz8fDz66KP48ssv4erqaliek5ODr7/+GnPmzEHv3r0RGRmJJUuWYPfu3di7d6+EFVNNfLnzPD7aeAYA8PqDYXiqRzOJKyIiooZE8iAzfvx49O/fHzExMUbL4+LiUFZWZrQ8LCwMgYGB2LNnT7XbKykpQW5urtGNpHExswCz1p8GAEx+oCWev7+5xBUREVFDo5DyxZctW4ZDhw7hwIEDlR5LTU2FUqmEi4uL0XJvb2+kpqZWu82ZM2dixowZ5i6VauHTrWeh1QncH+qJl6JDpC6HiIgaIMlaZC5fvowJEybgxx9/hJ2dndm2O2XKFOTk5Bhuly9fNtu2yXQXMguw+vAVAMDEmJYSV0NERA2VZEEmLi4O6enp6NixIxQKBRQKBXbs2IFPPvkECoUC3t7eKC0tRXZ2ttHz0tLS4OPjU+12VSoVnJ2djW5U9z7dmgidAHqHeaF9gIvU5RARUQMlWddSdHQ0jh07ZrRs7NixCAsLw+uvv46AgADY2tpiy5YtGDp0KAAgISEBSUlJiIqKkqJkMtH5jHysOXwVADCBXUpERGRBkgUZJycntGnTxmiZg4MD3N3dDcuffPJJvPLKK3Bzc4OzszNeeuklREVFoWvXrlKUTCb6bOtZ6AQQHeaFdmyNISIiC5J0sO+dzJ07F3K5HEOHDkVJSQliY2OxcOFCqcui2ziXkY818TdaY2LYGkNERJYlE0IIqYuwpNzcXGg0GuTk5HC8TB2YtDweqw9fRUwrL3w1upPU5RARkZUy9ftb8nlkqOE4l5GP3/StMdE8U4mIiCyPQYbM5tMtFWcqxbTyRlt/jdTlEBFRI8AgQ2ZxNj0fvx9JBgBM5NgYIiKqIwwyZBb6eWMeCPdGmyZsjSEiorrBIEN37Wx6nqE1hvPGEBFRXWKQobv2yZazEALow9YYIiKqYwwydFcS0/Kw9uiN1hiOjSEiojrGIEN35ZOtFa0xsa290dqPrTFERFS3GGSo1s6k5eEPfWsM540hIiIJMMhQreSXlGPOxjMQAniwtQ/C/ThrMhER1b16fa0lkl5WQSnOZuQjMS0fZ9PzkZieh3Pp+UjOKTasw7ExREQkFQYZqiQlpwhvrTmB+MtZyMwvrXY9D0cVRnUOQCtftsYQEZE0GGTIiE4nMGl5PPaev25Y1sRFjRZejgjxckSLm24u9koJKyUiImKQoVss3X0Re89fh9rWBt+M6YQIfw0cVPyYEBFR/cRvKDI4m56PD9afBgC82b8Vopq7S1wRERHR7fGsJQIAlGt1mLzyCErKdegR4oHHugRKXRIREdEdMcgQAGDxjnM4cjkbTnYKzB4WAZlMJnVJREREd8QgQziRnIP5WxIBADMebg1fjVriioiIiEzDINPIlZRrMXnFEZRpBWJbe2NwhyZSl0RERGQyBplGbt7mRJxOzYO7gxLvDW7LLiUiIrIqDDKNWNylLHy+4xwA4L3BbeHhqJK4IiIiopphkGmkCkvL8erKI9AJYEiHJniwjY/UJREREdUYg0wj9cG607iQWQAfZztMe7i11OUQERHVCoNMI7TrbCa+3XMJADB7WAQ0aluJKyIiIqodBplGJre4DP9ZeQQA8GiXQNzX0lPiioiIiGqPQaaRWbT9HJJzihHoZo83+7WSuhwiIqK7wiDTiAghsPZIMgDg9QfDeDFIIiKyegwyjciJ5FxcySqCna0cvcLYpURERNaPQaYR2XAiFQDQs6Un7JVsjSEiIuvHINOIrD9eEWT6tvGVuBIiIiLzYJBpJM6m5yMxPR+2NjL0CvOSuhwiIiKzYJBpJPTdSvc29+C8MURE1GAwyDQS+iDDSxEQEVFDwiDTCFzNLsLRKzmQyYAHwr2lLoeIiMhsGGQagQ03Bvl2aurGK1wTEVGDwiDTCKzXdyu1ZrcSERE1LAwyDVxGXgkOXLwOAIjl+BgiImpgGGQauE0n0yAEEOGvQRMXtdTlEBERmRWDTAOn71aKZbcSERE1QAwyDVhOURl2n80EAPRltxIRETVADDIN2NbTaSjXCbT0dkQzT0epyyEiIjI7BpkGTH9tJZ6tREREDRWDTANVWFqOHWcyAPBsJSIiargYZBqonWcyUFymQ4CbGuG+zlKXQ0REZBEMMg3Uzd1KMplM4mqIiIgsg0GmASot12HLqXQAvEgkERE1bAwyDdCuc5nIKymHp5MKHQJcpS6HiIjIYhhkGiD9RSJjW3tDLme3EhERNVw1DjJLlizBypUrKy1fuXIlvv32W7MURbWn1QlsPJkGAHiwta/E1RAREVlWjYPMzJkz4eHhUWm5l5cX3n//fbMURbV34OJ1XC8ohYu9Lbo0c5O6HCIiIouqcZBJSkpCcHBwpeVBQUFISkoyS1FUe/qzlWJaecPWhj2HRETUsNX4m87LywtHjx6ttPzIkSNwd3c3S1FUO0IIbDjB2XyJiKjxqHGQGTVqFF5++WVs27YNWq0WWq0WW7duxYQJEzBy5EhL1EgmOnolByk5xbBX2qB7SOXuPyIiooZGUdMn/O9//8PFixcRHR0NhaLi6TqdDk888QTHyEhs/Y3WmF5hXrCztZG4GiIiIsurcZBRKpVYvnw53n33XcTHx0OtVqNt27YICgqyRH1kopyiMqw4cBkAu5WIiKjxqHGQ0QsJCUFISIg5a6G7MG/zGVwrKEVzTwfEMsgQEVEjUeMxMkOHDsUHH3xQafns2bPxyCOPmKUoqpkzaXn4bs8lAMD0h1tDqeDZSkRE1DjU+Btv586d6NevX6Xlffv2xc6dO81SFJlOCIHpv5+AVifQJ9wbPUI8pS6JiIioztQ4yOTn50OpVFZabmtri9zcXLMURaZbfzwVu89dg1Ihx1sPhUtdDhERUZ2qcZBp27Ytli9fXmn5smXLEB7OL9K6VFSqxbt/ngIAPHdfMwS42UtcERERUd2q8WDft956C0OGDMG5c+fQu3dvAMCWLVvw888/V3kNJrKcxTvO4Wp2Efw0dnj+/hZSl0NERFTnahxkBgwYgDVr1uD999/HqlWroFarERERgc2bN6Nnz56WqJGqcPl6IRbvOAcAmNo/HGol540hIqLGp1anX/fv3x/9+/c3dy1UA+/9eQol5TpENXNHv7Y83ZqIiBonSc/TXbRoESIiIuDs7AxnZ2dERUVh3bp1hseLi4sxfvx4uLu7w9HREUOHDkVaWpqEFdcP/yRmYv2JVNjIZZj+cGvIZDKpSyIiIpJEjYOMVqvFRx99hM6dO8PHxwdubm5Gt5rw9/fHrFmzEBcXh4MHD6J3794YOHAgTpw4AQCYNGkS1q5di5UrV2LHjh1ITk7GkCFDalpyg1Km1WH62or983jXIIT6OElcERERkXRqHGRmzJiBOXPmYMSIEcjJycErr7yCIUOGQC6XY/r06TXa1oABA9CvXz+EhISgZcuWeO+99+Do6Ii9e/ciJycHX3/9NebMmYPevXsjMjISS5Yswe7du7F3796alt1gfLv7Is6m58PNQYlJD7SUuhwiIiJJ1TjI/Pjjj/jyyy8xefJkKBQKjBo1Cl999RXefvvtuwoYWq0Wy5YtQ0FBAaKiohAXF4eysjLExMQY1gkLC0NgYCD27NlT69exZhl5JZi/OREA8FpsKDRqW4krIiIiklaNB/umpqaibdu2AABHR0fk5OQAAB566CG89dZbNS7g2LFjiIqKQnFxMRwdHbF69WqEh4cjPj4eSqUSLi4uRut7e3sjNTW12u2VlJSgpKTEcL8hTdI3e/1p5JWUI8Jfg+H3BEhdDhERkeRq3CLj7++PlJQUAEDz5s2xceNGAMCBAwegUqlqXEBoaCji4+Oxb98+PP/88xg9ejROnjxZ4+3ozZw5ExqNxnALCGgYX/jxl7OxMu4KgIrrKcnlHOBLRERU4yAzePBgbNmyBQDw0ksv4a233kJISAieeOIJjBs3rsYFKJVKtGjRApGRkZg5cybatWuH+fPnw8fHB6WlpcjOzjZaPy0tDT4+1Z9uPGXKFOTk5Bhuly9frnFN9dE7Nwb4Du3oj46BrhJXQ0REVD/UuGtp1qxZhv+PGDECQUFB2L17N0JCQjBgwIC7Lkin06GkpASRkZGwtbXFli1bMHToUABAQkICkpKSEBUVVe3zVSpVrVqG6rOsglIcSsoGALz+YKi0xRAREdUjtZoQ72Zdu3ZF165dKy3v378/vvrqK/j6+lb73ClTpqBv374IDAxEXl4efvrpJ2zfvh0bNmyARqPBk08+iVdeeQVubm5wdnbGSy+9hKioqCpfryE7nZoHAAhwU8PL2U7iaoiIiOqPuw4y1dm5cyeKiopuu056ejqeeOIJpKSkQKPRICIiAhs2bMADDzwAAJg7dy7kcjmGDh2KkpISxMbGYuHChZYqud46k1YRZEK9OWcMERHRzSwWZEzx9ddf3/ZxOzs7LFiwAAsWLKijiuonfYsMJ78jIiIyJuklCsg0CakVp5CH+jhLXAkREVH9wiBTzwkhcCYtHwC7loiIiG7FIFPPXc0uQn5JOWxtZGjm6SB1OURERPUKg0w9l3BjfExzT0fY2vDHRUREdLMafzPu3LkT5eXllZaXl5dj586dhvtvvvlmja+GTZUl3DhjqSW7lYiIiCqpcZDp1asXrl+/Xml5Tk4OevXqZbg/ZcqUStdJoppL4BlLRERE1apxkBFCQCarfJ2fa9euwcGBYzjMTR9kwhhkiIiIKjF5HpkhQ4YAAGQyGcaMGWN0GQCtVoujR4/i3nvvNX+FjViZVodzGRVnLLFriYiIqDKTg4xGowFQ0SLj5OQEtVpteEypVKJr1654+umnzV9hI3YhswBlWgFHlQL+ruo7P4GIiKiRMTnILFmyBADQtGlTvPrqq+xGqgP6GX1bejtW2Z1HRETU2NV4jMxrr71m9KV66dIlzJs3Dxs3bjRrYQScMQz05Yy+REREValxkBk4cCC+++47AEB2djY6d+6Mjz/+GAMHDsSiRYvMXmBjZrjGkrejxJUQERHVTzUOMocOHUKPHj0AAKtWrYKPjw8uXbqE7777Dp988onZC2zMEtJ4jSUiIqLbqXGQKSwshJNTxRk0GzduxJAhQyCXy9G1a1dcunTJ7AU2Vvkl5bh8vQgA55AhIiKqTo2DTIsWLbBmzRpcvnwZGzZsQJ8+fQAA6enpcHZmy4G5JN6Y0dfTSQU3B6XE1RAREdVPNQ4yb7/9Nl599VU0bdoUnTt3RlRUFICK1pkOHTqYvcDGihPhERER3ZnJp1/rDRs2DN27d0dKSgratWtnWB4dHY3BgwebtbjG7N+BvgwyRERE1anV5ZR9fHzg5OSETZs2oaioYhxHp06dEBYWZtbiGrMz+otFskWGiIioWjUOMteuXUN0dDRatmyJfv36ISUlBQDw5JNPYvLkyWYvsLFi1xIREdGd1TjITJo0Cba2tkhKSoK9vb1h+YgRI7B+/XqzFtdYZeSV4FpBKWQyIMSLQYaIiKg6NR4js3HjRmzYsAH+/v5Gy0NCQnj6tZnou5WaujtArbSRuBoiIqL6q8YtMgUFBUYtMXrXr183uiI21d7N11giIiKi6tU4yPTo0cNwiQIAkMlk0Ol0mD17Nnr16mXW4hqrhFTO6EtERGSKGnctzZ49G9HR0Th48CBKS0vx2muv4cSJE7h+/Tp27dpliRobnYS0fAAc6EtERHQnNW6RcXZ2xqlTp9C9e3cMHDgQBQUFGDJkCA4fPgxbW1tL1Nio6HTCMKtvS84hQ0REdFs1bpEJDg5GSkoKpk6darT82rVr8Pf3h1arNVtxjdHlrEIUlmqhVMjR1L3yWCQiIiL6V41bZIQQVS7Pz8+HnZ3dXRfU2OkH+oZ4OUJhU6v5ComIiBoNk1tkXnnlFQAVg3vffvttozOXtFot9u3bh/bt25u9wMbmDC9NQEREZDKTg8zhw4cBVLTIHDt2DErlv1dkViqVaNeuHV599VXzV9jInL4xPiaUA32JiIjuyOQgs23bNgDA2LFjMX/+fDg789RgS9BfmoBBhoiI6M5qPNh3yZIllqiDAJSUa3EhswAAgwwREZEpOJq0HjmXXgCtTsDZTgEfZw6cJiIiuhMGmXokIa1iRt8wH2fIZDKJqyEiIqr/GGTqkYTUihl92a1ERERkGgaZekR/jaWWDDJEREQmYZCpR/RnLPEaS0RERKZhkKkncovLkJxTDIDXWCIiIjIVg0w9oZ/R11djB42aF98kIiIyBYNMPXGaE+ERERHVGINMPcEZfYmIiGqOQaaeSEjjxSKJiIhqikGmHhBCsEWGiIioFhhk6oG03BLkFJXBRi5DCy9HqcshIiKyGgwy9YC+WynYwwEqhY3E1RAREVkPBpl6QD+jL8fHEBER1QyDTD3AU6+JiIhqh0GmHjiTxiBDRERUGwwyEtPqBBLTblz1ml1LRERENcIgI7Gz6fkoKddBbWuDQDd7qcshIiKyKgwyEvvzaDIAoEszN8jlMomrISIisi4MMhLS6QR+PXwVADCko7/E1RAREVkfBhkJHbh4HVeyiuCkUqBPuLfU5RAREVkdBhkJ/XLoCgCgX1tf2NlyIjwiIqKaYpCRSFGpFn8dSwUADOnYROJqiIiIrBODjEQ2nkxFfkk5/F3V6NTUTepyiIiIrBKDjER+PXRjkG+HJjxbiYiIqJYYZCSQnluMvxMzAACDebYSERFRrTHISOC3+GToBBAZ5IpgDwepyyEiIrJaDDIS0J+txEG+REREd4dBpo6dTM7F6dQ8KG3keKitn9TlEBERWTUGmTr2643WmJhwL2jsbSWuhoiIyLoxyNShcq0Oa+Irrq00pAMH+RIREd0tBpk69PfZTGTml8DNQYmeoZ5Sl0NERGT1GGTq0C9xFd1KD7fzg60Ndz0REdHdkvTbdObMmejUqROcnJzg5eWFQYMGISEhwWid4uJijB8/Hu7u7nB0dMTQoUORlpYmUcW1l1NUho0nK+oeyrljiIiIzELSILNjxw6MHz8ee/fuxaZNm1BWVoY+ffqgoKDAsM6kSZOwdu1arFy5Ejt27EBycjKGDBkiYdW1s+5YCkrLdQjxckSbJs5Sl0NERNQgKKR88fXr1xvdX7p0Kby8vBAXF4f77rsPOTk5+Prrr/HTTz+hd+/eAIAlS5agVatW2Lt3L7p27SpF2bViuCRBR3/IZLwkARERkTnUq4EaOTk5AAA3t4qLKMbFxaGsrAwxMTGGdcLCwhAYGIg9e/ZUuY2SkhLk5uYa3aSWdK0Q+y9eh0wGDOrAuWOIiIjMpd4EGZ1Oh4kTJ6Jbt25o06YNACA1NRVKpRIuLi5G63p7eyM1NbXK7cycORMajcZwCwgIsHTpd7T6cEVrTLfmHvDVqCWuhoiIqOGoN0Fm/PjxOH78OJYtW3ZX25kyZQpycnIMt8uXL5upwtoRQuDXw7wkARERkSVIOkZG78UXX8Qff/yBnTt3wt//3zN6fHx8UFpaiuzsbKNWmbS0NPj4+FS5LZVKBZVKZemSTXYoKQuXrhXCXmmD2NZV10xERES1I2mLjBACL774IlavXo2tW7ciODjY6PHIyEjY2tpiy5YthmUJCQlISkpCVFRUXZdbK7/cGOT7YBsfOKjqRW4kIiJqMCT9Zh0/fjx++ukn/Pbbb3BycjKMe9FoNFCr1dBoNHjyySfxyiuvwM3NDc7OznjppZcQFRVlFWcsFZdp8ceRiksSDOPcMURERGYnaZBZtGgRAOD+++83Wr5kyRKMGTMGADB37lzI5XIMHToUJSUliI2NxcKFC+u40trZeSYDucXl8NPYoWszd6nLISIianAkDTJCiDuuY2dnhwULFmDBggV1UJF5ncuomNivSzN3yOWcO4aIiMjc6s1ZSw1Rel4xAMDLqf4MPiYiImpIGGQsKD2vBADgySBDRERkEQwyFpSRWxFkvJztJK6EiIioYWKQsSB2LREREVkWg4wF6buWGGSIiIgsg0HGQvJLylFYqgXAriUiIiJLYZCxkPTcim4lB6UNHDmjLxERkUUwyFiIoVuJrTFEREQWwyBjITz1moiIyPIYZCxE37XEgb5ERESWwyBjIRmGM5bYtURERGQpDDIW8u8YGbbIEBERWQqDjIVwMjwiIiLLY5CxkPRcdi0RERFZGoOMhbBriYiIyPIYZCyguEyLnKIyAOxaIiIisiQGGQvQn7GkVMihUdtKXA0REVHDxSBjAYbJ8BxVkMlkEldDRETUcDHIWECG/owljo8hIiKyKAYZC0gznLHEIENERGRJDDIW8O8cMjz1moiIyJIYZCwgnS0yREREdYJBxgI4hwwREVHdYJCxgHReMJKIiKhOMMhYgP6sJU92LREREVkUg4yZlWt1uFZQCgDwdmaLDBERkSUxyJhZZn4phABs5DK4OyilLoeIiKhBY5AxM/2p1x6OSsjlnNWXiIjIkhhkzOzfU6/ZrURERGRpDDJm9u8ZSxzoS0REZGkMMmaWzussERER1RkGGTMzXPmaXUtEREQWxyBjZrw8ARERUd1hkDGzDMMFIxlkiIiILI1Bxsz+vc4Su5aIiIgsjUHGjHQ6gQyetURERFRnGGTMKKuwFOU6AQDwcGSQISIisjQGGTPSdyu5OSihVHDXEhERWRq/bc2Ik+ERERHVLQYZM0rPrThjyZNBhoiIqE4wyJjRvy0yPGOJiIioLjDImJG+RYaXJyAiIqobDDJmxDEyREREdYtBxozYtURERFS3GGTMSH/la292LREREdUJBhkzEULcdMFItsgQERHVBQYZM8ktLkdJuQ4AB/sSERHVFQYZM9Ff9drJTgE7WxuJqyEiImocGGTM5N9uJbbGEBER1RUGGTPhGUtERER1j0HGTPRnLHF8DBERUd1hkDETdi0RERHVPQYZM2HXEhERUd1jkDETdi0RERHVPQYZM9G3yHiya4mIiKjOMMiYSQZn9SUiIqpzDDJmUFSqRV5JOQB2LREREdUlBhkz0I+PsbOVw0mlkLgaIiKixoNBxgxuPmNJJpNJXA0REVHjwSBjBpxDhoiISBoMMmbAU6+JiIikwSBjBpwMj4iISBoMMmaQllvRIsM5ZIiIiOqWpEFm586dGDBgAPz8/CCTybBmzRqjx4UQePvtt+Hr6wu1Wo2YmBgkJiZKU+xtZORxjAwREZEUJA0yBQUFaNeuHRYsWFDl47Nnz8Ynn3yCxYsXY9++fXBwcEBsbCyKi4vruNLb0w/29XZm1xIREVFdknTSk759+6Jv375VPiaEwLx58/Df//4XAwcOBAB899138Pb2xpo1azBy5Mi6LPW2ONiXiIhIGvV2jMyFCxeQmpqKmJgYwzKNRoMuXbpgz549ElZmrLRch6zCMgAc7EtERFTX6u00tKmpqQAAb29vo+Xe3t6Gx6pSUlKCkpISw/3c3FzLFHhDRn7Fa9nayOBqb2vR1yIiIiJj9bZFprZmzpwJjUZjuAUEBFj09dL1Zyw5qjirLxERUR2rt0HGx8cHAJCWlma0PC0tzfBYVaZMmYKcnBzD7fLlyxatUz+HjCcH+hIREdW5ehtkgoOD4ePjgy1bthiW5ebmYt++fYiKiqr2eSqVCs7OzkY3S0rnqddERESSkXSMTH5+Ps6ePWu4f+HCBcTHx8PNzQ2BgYGYOHEi3n33XYSEhCA4OBhvvfUW/Pz8MGjQIOmKvkXGja4lBhkiIqK6J2mQOXjwIHr16mW4/8orrwAARo8ejaVLl+K1115DQUEBnnnmGWRnZ6N79+5Yv3497OzqTzcOL09AREQkHUmDzP333w8hRLWPy2QyvPPOO3jnnXfqsKqaMQQZziFDRERU5+rtGBlrYZgMj11LREREdY5B5i7pL0/AriUiIqK6xyBzF7Q6gcx8di0RERFJhUHmLlwrKIFOADIZ4O6glLocIiKiRodB5i7ou5XcHVRQ2HBXEhER1TV++96FDE6GR0REJCkGmbtgOGOJ42OIiIgkwSBzF/49Y4lBhoiISAoMMneBs/oSERFJi0HmLqTduM6SN7uWiIiIJMEgcxf0LTKebJEhIiKSBIPMXcjgdZaIiIgkxSBTS0IInn5NREQkMQaZWsouLEOpVgcA8GSQISIikgSDTC3px8e42NtCpbCRuBoiIqLGiUGmlgyT4bE1hoiISDIMMrX072R4PGOJiIhIKgwytZTOgb5ERESSY5CpJX3XkidPvSYiIpIMg0wtlWsFlDZydi0RERFJSCaEEFIXYUm5ubnQaDTIycmBs7OzWbcthIBWJ6CwYR4kIiIyJ1O/vxV1WFODI5PJoLCRSV0GERFRo8WmBCIiIrJaDDJERERktRhkiIiIyGoxyBAREZHVYpAhIiIiq8UgQ0RERFaLQYaIiIisFoMMERERWS0GGSIiIrJaDDJERERktRhkiIiIyGoxyBAREZHVYpAhIiIiq9Xgr34thABQcTlwIiIisg76723993h1GnyQycvLAwAEBARIXAkRERHVVF5eHjQaTbWPy8Sdoo6V0+l0SE5OhpOTE2Qymdm2m5ubi4CAAFy+fBnOzs5m2y5Vjfu7bnF/1z3u87rF/V23arO/hRDIy8uDn58f5PLqR8I0+BYZuVwOf39/i23f2dmZvwR1iPu7bnF/1z3u87rF/V23arq/b9cSo8fBvkRERGS1GGSIiIjIajHI1JJKpcK0adOgUqmkLqVR4P6uW9zfdY/7vG5xf9ctS+7vBj/Yl4iIiBoutsgQERGR1WKQISIiIqvFIENERERWi0GGiIiIrBaDTC0tWLAATZs2hZ2dHbp06YL9+/dLXVKDsHPnTgwYMAB+fn6QyWRYs2aN0eNCCLz99tvw9fWFWq1GTEwMEhMTpSm2AZg5cyY6deoEJycneHl5YdCgQUhISDBap7i4GOPHj4e7uzscHR0xdOhQpKWlSVSxdVu0aBEiIiIMk4JFRUVh3bp1hse5ry1n1qxZkMlkmDhxomEZ97d5TZ8+HTKZzOgWFhZmeNxS+5tBphaWL1+OV155BdOmTcOhQ4fQrl07xMbGIj09XerSrF5BQQHatWuHBQsWVPn47Nmz8cknn2Dx4sXYt28fHBwcEBsbi+Li4jqutGHYsWMHxo8fj71792LTpk0oKytDnz59UFBQYFhn0qRJWLt2LVauXIkdO3YgOTkZQ4YMkbBq6+Xv749Zs2YhLi4OBw8eRO/evTFw4ECcOHECAPe1pRw4cACff/45IiIijJZzf5tf69atkZKSYrj9888/hscstr8F1Vjnzp3F+PHjDfe1Wq3w8/MTM2fOlLCqhgeAWL16teG+TqcTPj4+4sMPPzQsy87OFiqVSvz8888SVNjwpKenCwBix44dQoiK/WtraytWrlxpWOfUqVMCgNizZ49UZTYorq6u4quvvuK+tpC8vDwREhIiNm3aJHr27CkmTJgghOBn2xKmTZsm2rVrV+VjltzfbJGpodLSUsTFxSEmJsawTC6XIyYmBnv27JGwsobvwoULSE1NNdr3Go0GXbp04b43k5ycHACAm5sbACAuLg5lZWVG+zwsLAyBgYHc53dJq9Vi2bJlKCgoQFRUFPe1hYwfPx79+/c32q8AP9uWkpiYCD8/PzRr1gyPPvookpKSAFh2fzf4i0aaW2ZmJrRaLby9vY2We3t74/Tp0xJV1TikpqYCQJX7Xv8Y1Z5Op8PEiRPRrVs3tGnTBkDFPlcqlXBxcTFal/u89o4dO4aoqCgUFxfD0dERq1evRnh4OOLj47mvzWzZsmU4dOgQDhw4UOkxfrbNr0uXLli6dClCQ0ORkpKCGTNmoEePHjh+/LhF9zeDDBEBqPjL9fjx40Z92mR+oaGhiI+PR05ODlatWoXRo0djx44dUpfV4Fy+fBkTJkzApk2bYGdnJ3U5jULfvn0N/4+IiECXLl0QFBSEFStWQK1WW+x12bVUQx4eHrCxsak00jotLQ0+Pj4SVdU46Pcv9735vfjii/jjjz+wbds2+Pv7G5b7+PigtLQU2dnZRutzn9eeUqlEixYtEBkZiZkzZ6Jdu3aYP38+97WZxcXFIT09HR07doRCoYBCocCOHTvwySefQKFQwNvbm/vbwlxcXNCyZUucPXvWop9vBpkaUiqViIyMxJYtWwzLdDodtmzZgqioKAkra/iCg4Ph4+NjtO9zc3Oxb98+7vtaEkLgxRdfxOrVq7F161YEBwcbPR4ZGQlbW1ujfZ6QkICkpCTuczPR6XQoKSnhvjaz6OhoHDt2DPHx8YbbPffcg0cffdTwf+5vy8rPz8e5c+fg6+tr2c/3XQ0VbqSWLVsmVCqVWLp0qTh58qR45plnhIuLi0hNTZW6NKuXl5cnDh8+LA4fPiwAiDlz5ojDhw+LS5cuCSGEmDVrlnBxcRG//fabOHr0qBg4cKAIDg4WRUVFEldunZ5//nmh0WjE9u3bRUpKiuFWWFhoWOe5554TgYGBYuvWreLgwYMiKipKREVFSVi19XrjjTfEjh07xIULF8TRo0fFG2+8IWQymdi4caMQgvva0m4+a0kI7m9zmzx5sti+fbu4cOGC2LVrl4iJiREeHh4iPT1dCGG5/c0gU0uffvqpCAwMFEqlUnTu3Fns3btX6pIahG3btgkAlW6jR48WQlScgv3WW28Jb29voVKpRHR0tEhISJC2aCtW1b4GIJYsWWJYp6ioSLzwwgvC1dVV2Nvbi8GDB4uUlBTpirZi48aNE0FBQUKpVApPT08RHR1tCDFCcF9b2q1BhvvbvEaMGCF8fX2FUqkUTZo0ESNGjBBnz541PG6p/S0TQoi7a9MhIiIikgbHyBAREZHVYpAhIiIiq8UgQ0RERFaLQYaIiIisFoMMERERWS0GGSIiIrJaDDJERERktRhkiKjRkclkWLNmjdRlEJEZMMgQUZ0aM2YMZDJZpduDDz4odWlEZIUUUhdARI3Pgw8+iCVLlhgtU6lUElVDRNaMLTJEVOdUKhV8fHyMbq6urgAqun0WLVqEvn37Qq1Wo1mzZli1apXR848dO4bevXtDrVbD3d0dzzzzDPLz843W+eabb9C6dWuoVCr4+vrixRdfNHo8MzMTgwcPhr29PUJCQvD7779b9k0TkUUwyBBRvfPWW29h6NChOHLkCB599FGMHDkSp06dAgAUFBQgNjYWrq6uOHDgAFauXInNmzcbBZVFixZh/PjxeOaZZ3Ds2DH8/vvvaNGihdFrzJgxA8OHD8fRo0fRr18/PProo7h+/Xqdvk8iMoO7vuwkEVENjB49WtjY2AgHBwej23vvvSeEqLgi93PPPWf0nC5duojnn39eCCHEF198IVxdXUV+fr7h8T///FPI5XKRmpoqhBDCz89PTJ06tdoaAIj//ve/hvv5+fkCgFi3bp3Z3icR1Q2OkSGiOterVy8sWrTIaJmbm5vh/1FRUUaPRUVFIT4+HgBw6tQptGvXDg4ODobHu3XrBp1Oh4SEBMhkMiQnJyM6Ovq2NURERBj+7+DgAGdnZ6Snp9f2LRGRRBhkiKjOOTg4VOrqMRe1Wm3Sera2tkb3ZTIZdDqdJUoiIgviGBkiqnf27t1b6X6rVq0AAK1atcKRI0dQUFBgeHzXrl2Qy+UIDQ2Fk5MTmjZtii1bttRpzUQkDbbIEFGdKykpQWpqqtEyhUIBDw8PAMDKlStxzz33oHv37vjxxx+xf/9+fP311wCARx99FNOmTcPo0aMxffp0ZGRk4KWXXsLjjz8Ob29vAMD06dPx3HPPwcvLC3379kVeXh527dqFl156qW7fKBFZHIMMEdW59evXw9fX12hZaGgoTp8+DaDijKJly5bhhRdegK+vL37++WeEh4cDAOzt7bFhwwZMmDABnTp1gr29PYYOHYo5c+YYtjV69GgUFxdj7ty5ePXVV+Hh4YFhw4bV3RskojojE0IIqYsgItKTyWRYvXo1Bg0aJHUpRGQFOEaGiIiIrBaDDBEREVktjpEhonqFvd1EVBNskSEiIiKrxSBDREREVotBhoiIiKwWgwwRERFZLQYZIiIisloMMkRERGS1GGSIiIjIajHIEBERkdVikCEiIiKr9f/K/bvoFNxYmQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_epoch_performancesCSV(csv_path, model_name, metric=\"test_acc\"): # metric can also be 'train_acc' or 'loss'\n",
    "    '''\n",
    "    Plots the performance of the model for each epoch.\n",
    "\n",
    "    Args:\n",
    "        epoch_performances (list): List of dictionaries of the performance of the model for each epoch (accuracy, precision, recall, f1).\n",
    "        metric (str): The metric to plot.\n",
    "    '''\n",
    "\n",
    "    y = pd.read_csv(csv_path)[metric].to_numpy()\n",
    "    plt.title(f\"Test {metric} over training epochs for model {model_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(f\"{metric}\")\n",
    "    plt.plot(y)\n",
    "    plt.show()\n",
    "\n",
    "plot_epoch_performancesCSV('./models/CONVNET2_MIDLRDECAY_B64.csv', 'ConvNetFinal', 'test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O4kI2zGvtF9"
   },
   "source": [
    "<a id=\"section-8\"></a>\n",
    "### **Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)**\n",
    "\n",
    "In this section, you will work with a subset of the [STL-10](https://cs.stanford.edu/~acoates/stl10/) dataset, containing higher resolution images and different object classes than CIFAR-100. Before fine-tuning your ConvNet on this dataset, first complete the `visualise_stl10` function to display sample images from the following 5 classes:\n",
    "\n",
    "1. **Bird**\n",
    "2. **Deer**\n",
    "3. **Dog**\n",
    "4. **Horse**\n",
    "5. **Monkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-12T13:48:07.603891Z"
    },
    "id": "04XTYWUDvtF9"
   },
   "outputs": [],
   "source": [
    "def visualise_stl10(class_mapping, train_set):\n",
    "    '''\n",
    "    Visualizes 5 images from each specified class in the STL-10 dataset.\n",
    "\n",
    "    Args:\n",
    "        class_mapping (dict): A dictionary mapping class indices to class names.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    data = torch.tensor(train_set.data)\n",
    "    labels = torch.tensor(train_set.labels)\n",
    "    fig, axs = plt.subplots(num_classes_per_superclass, num_samples, figsize=(25, 20))\n",
    "\n",
    "    for index, label in enumerate(class_mapping.keys()):\n",
    "        subset = data[labels == label]\n",
    "        random_indices = torch.randperm(subset.size(0))\n",
    "        \n",
    "        # Set the class name as the label for the first column\n",
    "        class_name = class_mapping[label]\n",
    "        axs[index, 0].set_ylabel(class_name, fontsize=16, rotation=0, labelpad=80, va='center')\n",
    "        \n",
    "        for num_sample, sample_index in enumerate(random_indices[ : num_samples]):\n",
    "            axs[index, num_sample].imshow(subset[sample_index].permute(1, 2, 0))\n",
    "            axs[index, num_sample].set_xticks([])\n",
    "            axs[index, num_sample].set_yticks([])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:48:07.610659Z",
     "start_time": "2024-10-12T13:48:07.606251Z"
    },
    "id": "JwIQHj3fvtF9"
   },
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# Load the STL-10 training set\n",
    "#train_set = torchvision.datasets.STL10(root='./data', download=True, transform=transform)\n",
    "\n",
    "# Define the class mapping for bird, deer, dog, horse, and monkey\n",
    "class_mapping = {1: 'bird', 4: 'deer', 5: 'dog', 6: 'horse', 7: 'monkey'}\n",
    "\n",
    "# Visualize STL-10 classes\n",
    "visualise_stl10(class_mapping, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdbpTzlJvtF9"
   },
   "source": [
    "After visualizing the data, implement the `STL10_loader` class to create a custom data loader that initializes the dataset, extracts the target classes, and applies the necessary image transformations. Once these tasks are completed, you will move on to fine-tuning the ConvNet on this dataset in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-12T13:48:07.608325Z"
    },
    "id": "xQArgILqvtF9"
   },
   "outputs": [],
   "source": [
    "class STL10_loader(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, download=False):\n",
    "        '''\n",
    "        Initializes the STL10 dataset.\n",
    "\n",
    "        Args:\n",
    "            root (str): Root directory of the dataset.\n",
    "            train (bool): If True, use the training set, otherwise use the test set.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.dataset = torchvision.datasets.STL10(root=root, split=('train' if train else 'test'), download=False, transform=transform)\n",
    "        self.labels = torch.tensor(self.dataset.labels)\n",
    "        target_classes = torch.tensor([1, 4, 5, 6, 7])\n",
    "\n",
    "        # Filter data and labels by target classes\n",
    "        self.filtered_indices = torch.isin(self.labels, target_classes)\n",
    "        self.filtered_data = torch.tensor(self.dataset.data[self.filtered_indices])\n",
    "        self.filtered_labels = self.labels[self.filtered_indices]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return len(self.filtered_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed image and its target label.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        image = self.filtered_data[idx]\n",
    "        label = self.filtered_labels[idx]\n",
    "\n",
    "        return image.float(), label.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# class STL10_loader(Dataset):\n",
    "#     def __init__(self, root, train=True, transform=None):\n",
    "#         '''\n",
    "#         Initializes the STL10 dataset.\n",
    "\n",
    "#         Args:\n",
    "#             root (str): Root directory of the dataset.\n",
    "#             train (bool): If True, use the training set, otherwise use the test set.\n",
    "#             transform (callable, optional): A function/transform to apply to the images.\n",
    "#         '''\n",
    "\n",
    "#         # YOUR CODE HERE\n",
    "#         self.root = root\n",
    "#         self.train = train\n",
    "#         self.transform = transform\n",
    "#         self.images = []\n",
    "#         self.labels = []\n",
    "\n",
    "#         CWD = Path(os.getcwd())\n",
    "#         ROOT = CWD/Path(self.root)\n",
    "        \n",
    "#         with open(ROOT/Path(f\"stl10_binary/{\"train\" if train else \"test\"}_X.bin\"), \"rb\") as f:\n",
    "#             num_imgs = 5000 if train else 8000\n",
    "#             imgs = np.fromfile(f, dtype=np.uint8)\n",
    "#             imgs = imgs.reshape((num_imgs, 3, 96, 96))\n",
    "#             imgs = np.transpose(imgs, (0, 2, 3, 1))\n",
    "#             for i in range(num_imgs):\n",
    "#                 self.images.append(Image.fromarray(imgs[i,:,:,:]))\n",
    "\n",
    "#         with open(ROOT/Path(f\"stl10_binary/{\"train\" if train else \"test\"}_y.bin\"), \"rb\") as f:\n",
    "#             self.labels = list(np.fromfile(f, dtype=np.uint8) - 1)\n",
    "\n",
    "#         labels_to_keep = [1,4,5,6,7]\n",
    "#         self.images = [img for img,lab in zip(self.images, self.labels) if lab in set(labels_to_keep)]\n",
    "#         self.labels = [lab for lab in self.labels if lab in set(labels_to_keep)]\n",
    "#         for idx in range(len(self.labels)):\n",
    "#             for i,j in enumerate(labels_to_keep):\n",
    "#                 self.labels[idx] = i if (self.labels[idx] == j) else self.labels[idx]\n",
    "            \n",
    "\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         '''\n",
    "#         Returns the number of samples in the dataset.\n",
    "#         '''\n",
    "\n",
    "#         # YOUR CODE HERE\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         '''\n",
    "#         Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "#         Args:\n",
    "#             idx (int): The index of the sample to retrieve.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: A tuple containing the transformed image and its target label.\n",
    "#         '''\n",
    "\n",
    "#         # YOUR CODE HERE\n",
    "#         if self.transform:\n",
    "#             return self.transform(self.images[idx]), self.labels[idx]\n",
    "#         else:\n",
    "#             to_tensor = transforms.ToTensor()\n",
    "#             return to_tensor(self.images[idx]), self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV2nmek8vtF-"
   },
   "source": [
    "<a id=\"section-9\"></a>\n",
    "### **Section 9: Fine-tuning ConvNet on STL-10 (14 points)**\n",
    "\n",
    "In this section, you will load the pre-trained parameters of the ConvNet (trained on CIFAR-100) and modify the output layer to adapt it to the new dataset containing 5 classes. You can either first load the pre-trained parameters and then modify the output layer, or change the output layer before loading the matched pre-trained parameters. Once modified, you will train the model and document the settings of hyperparameters, accuracy, and learning curve. Additionally, visualize both the training loss and accuracy to assess the learning process. To gain a deeper understanding of the feature learning process, consider using techniques like [**t-sne**](https://lvdmaaten.github.io/tsne/) for feature space visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-12T13:48:07.610543Z"
    },
    "id": "o6-uHSxQvtF-"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Tuned model CNN performance\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "batch_size = 512\n",
    "extra_layers = True\n",
    "transformations = True\n",
    "weight_decay = 0.8\n",
    "nonlinear_fn = \"relu\"\n",
    "\n",
    "train_data_loader_cnn, test_data_loader_cnn = create_dataloaders(additional_transform=transformations, batch_size=batch_size, use_cache=True, dataset='STL10')\n",
    "\n",
    "filtered_state_dict = {k: v for k, v in conv_net.state_dict().items() if k not in ['model.20.weight', 'model.20.bias']}\n",
    "new_conv_net.load_state_dict(filtered_state_dict, strict=False)\n",
    "new_conv_net = ConvNet(extra_layers=extra_layers, num_classes=5)\n",
    "new_conv_net.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = create_optimizer(new_conv_net, learning_rate, weight_decay)\n",
    "\n",
    "cnn_epoch_performances, cnn_epoch_losses = train(new_conv_net, train_data_loader_cnn, test_data_loader_cnn, criterion, optimizer_cnn, epochs)\n",
    "plot_epoch_performances(cnn_epoch_performances, model_name=\"CNN\")\n",
    "plot_epoch_performances(cnn_epoch_losses, model_name=\"CNN\", metric=\"loss\")\n",
    "\n",
    "test_accuracy_cnn = validate(new_conv_net, test_data_loader_cnn)\n",
    "print(\"CNN test accuracy: \", test_accuracy_cnn)\n",
    "\n",
    "test_accuracy_cnn_per_class = validate_per_class(new_conv_net, test_data_loader_cnn, classes)\n",
    "print(\"Test Accuracy CNN per class: \", test_accuracy_cnn_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv_net.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkMVyK6UvtF-"
   },
   "source": [
    "<a id=\"section-10\"></a>\n",
    "### **Section 10: Bonus Challenge (optional)**\n",
    "\n",
    "Try to achieve the highest possible accuracy on the test dataset (5 classes from STL-10) by adjusting hyperparameters, modifying architectures, or applying techniques like data augmentation. The top-performing teams will earn bonus points that can significantly boost their final lab grade, even allowing it to exceed 10 (up to 11):\n",
    "\n",
    "- **1st place:** +1.0 to the final grade of the final lab\n",
    "- **2nd place:** +0.8 to the final grade of the final lab\n",
    "- **3rd place:** +0.6 to the final grade of the final lab\n",
    "- **4th place:** +0.4 to the final grade of the final lab\n",
    "- **5th place:** +0.2 to the final grade of the final lab\n",
    "\n",
    "**Hint:** You may use techniques like data augmentation, freezing early layers, modifying architecture, or optimizing hyperparameters. Only data from CIFAR-100 and STL-10 can be used, and you cannot add more than 3 additional convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:48:07.613122Z",
     "start_time": "2024-10-12T13:48:07.612859Z"
    },
    "id": "xB4qfUiHvtF-"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp7vymljvtF-"
   },
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFlrdZkcvtF-"
   },
   "source": [
    "| Name                   | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "|------------------------|--------------------------|-----------------------------|-------------------------|\n",
    "| Pradyut Nair           | 25 %                     | 25 %                        | 25 %                    |\n",
    "| Benjamin Shaffrey      | 25 %                     | 25 %                        | 25 %                    |\n",
    "| Akshay Sardjoe Missier | 25 %                     | 25 %                        | 25 %                    |\n",
    "| Christina Isaicu       | 25 %                      | 25 %                         | 25 %                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twW0FHwtvtF_"
   },
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
